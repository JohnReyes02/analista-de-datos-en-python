[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An√°lisis Exploratorio de Datos en Python",
    "section": "",
    "text": "Bienvenida\nEste curso interactivo cubre el proceso de exploraci√≥n y an√°lisis de datos en Python, desde entender un nuevo dataset hasta la limpieza e imputaci√≥n de valores.\nüìä Nivel: Intermedio\nüïí Duraci√≥n estimada: 4 horas\nüé• Incluye c√≥digo, visualizaciones y ejercicios",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#m√≥dulos-del-curso",
    "href": "index.html#m√≥dulos-del-curso",
    "title": "An√°lisis Exploratorio de Datos en Python",
    "section": "M√≥dulos del curso",
    "text": "M√≥dulos del curso\n\nConocer un conjunto de datos\n\nLimpieza e imputaci√≥n de datos\nRelaciones en los datos\nConvertir el an√°lisis exploratorio en acci√≥n",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "An√°lisis Exploratorio de Datos en Python",
    "section": "Datasets",
    "text": "Datasets\nEste curso utiliza los siguientes archivos:\n\nunemployment.csv\ndata_science_salaries.csv\nbooks.csv\ndivorce.csv\nplanes.csv",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html",
    "href": "01_Conocer_un_conjunto_de_datos.html",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "Exploraci√≥n inicial\n¬øCu√°l el la mejor manera de abordar un nuevo conjunto de datos? Aprende a validar y resumir datos categ√≥ricos y num√©ricos y a crear visualizaciones Seaborn para comunicar tus conclusiones.\nbooks = pd.read_csv('books.csv')\nbooks.head()\nbooks.info()\nbooks.value_counts('genre')\nbooks.describe()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.histplot(data=books, x='rating')\nplt.show()\nsns.histplot(data=books, x='rating', binwidth=.1)\nplt.show()",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#exploraci√≥n-inicial",
    "href": "01_Conocer_un_conjunto_de_datos.html#exploraci√≥n-inicial",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "An√°lisis Exploratorio de Datos\n\nEs el proceso de limpiar y revisar datos para:\n\nObterner informaci√≥n (Estad√≠stica descriptiva, correlaciones)\nGenrar hip√≥tesis\n\n\nUna primera mirada con .head()\n\n\n\n\nReuniendo m√°s .info()\n\n\n\n\nUna mirada cercana a las columnas categ√≥ricas\n\n\n\n\nColumnas num√©ricas con .describe()\n\n\n\n\nVisualizando datos num√©ricos\n\n\n\n\nAjustando la anchura del bin\n\n\n\n\nFunciones para la exploraci√≥n inicial\nEst√°s investigando las tasas de desempleo en todo el mundo y te han dado un nuevo conjunto de datos con el que trabajar. Los datos se han guardado y cargado para ti como un DataFrame de pandas llamado unemployment. Nunca antes hab√≠as visto los datos, as√≠ que tu primera tarea es utilizar unas cuantas funciones de pandas para conocer estos nuevos datos.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\n\n\nInstrucciones\n\nUtiliza una funci√≥n de pandas para imprimir las cinco primeras filas del DataFrame unemployment.\n\n\n# Print the first five rows of unemployment\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nUtiliza una funci√≥n pandas para imprimir un resumen de los valores y tipos de datos de las columnas que no faltan del DataFrame unemployment.\n\n\n# Print a summary of non-missing values and data types in the unemployment DataFrame]\nprint(unemployment.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 182 entries, 0 to 181\nData columns (total 15 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   country_code  182 non-null    object \n 1   country_name  182 non-null    object \n 2   continent     177 non-null    object \n 3   2010          182 non-null    float64\n 4   2011          182 non-null    float64\n 5   2012          182 non-null    float64\n 6   2013          182 non-null    float64\n 7   2014          182 non-null    float64\n 8   2015          182 non-null    float64\n 9   2016          182 non-null    float64\n 10  2017          182 non-null    float64\n 11  2018          182 non-null    float64\n 12  2019          182 non-null    float64\n 13  2020          182 non-null    float64\n 14  2021          182 non-null    float64\ndtypes: float64(12), object(3)\nmemory usage: 21.5+ KB\nNone\n\n\n\nImprime las estad√≠sticas de resument (recuento, media, desviaci√≥n est√°ndar, valores m√≠nimo, m√°ximo y cuartil) de cada columna num√©rica en unemployment.\n\n\n# Print summary statistics for numerical columns in unemployment\nprint(unemployment.describe())\n\n             2010        2011        2012        2013        2014        2015  \\\ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000   \nmean     8.409286    8.315440    8.317967    8.344780    8.179670    8.058901   \nstd      6.248887    6.266795    6.367270    6.416041    6.284241    6.161170   \nmin      0.450000    0.320000    0.480000    0.250000    0.200000    0.170000   \n25%      4.015000    3.775000    3.742500    3.692500    3.625000    3.662500   \n50%      6.965000    6.805000    6.690000    6.395000    6.450000    6.170000   \n75%     10.957500   11.045000   11.285000   11.310000   10.695000   10.215000   \nmax     32.020000   31.380000   31.020000   29.000000   28.030000   27.690000   \n\n             2016        2017        2018        2019        2020        2021  \ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000  \nmean     7.925879    7.668626    7.426429    7.243736    8.420934    8.390879  \nstd      6.045439    5.902152    5.818915    5.696573    6.040915    6.067192  \nmin      0.150000    0.140000    0.110000    0.100000    0.210000    0.260000  \n25%      3.800000    3.690000    3.625000    3.487500    4.285000    4.335000  \n50%      5.925000    5.650000    5.375000    5.240000    6.695000    6.425000  \n75%     10.245000   10.315000    9.257500    9.445000   11.155000   10.840000  \nmax     26.540000   27.040000   26.910000   28.470000   29.220000   33.560000  \n\n\nAhora haz aprendido que unemployment contiene 182 filas de datos de pa√≠ses, incluyendo country_code, country_name, continent y porcentajes de desempleo desde 2010 hasta 2021. ¬°Si miraste muy de cerca, podr√≠as haber notado que a algunos pa√≠ses les falta informaci√≥n en la columna continent! Continuemos explorando estos datos en el pr√≥ximo ejercicio.\n\n\n\nContar valores categ√≥ricos\nRecordemos del ejercicio anterior que el DataFrame unemployment contiene 182 filas de datos de pa√≠ses que incluyen country_code, country_name, continent y porcentajes de desempleo de 2010 a 2021.\nAhora vas a explorar los datos categ√≥ricos contenidos en unemployment para comprender los datos que contiene relacionados con cada continente.\n\nInstrucciones\n\nUtiliza un m√©todo para contar los valores asociados a cada continent en el DataFrame unemployment.\n\n\n# Count the values associated with each continent in unemployment\nprint(unemployment['continent'].value_counts())\n\ncontinent\nAfrica           53\nAsia             47\nEurope           39\nNorth America    18\nSouth America    12\nOceania           8\nName: count, dtype: int64\n\n\n¬øSab√≠as que hay 23 pa√≠ses en Am√©rica del Norte, que incluye pa√≠ses en el Caribe y Am√©rica Central? Puede que hayas notado que Am√©rica del Norte tiene 18 puntos de datos en el DataFrame unemployment, por lo que nos falta informaci√≥n de algunos de los pa√≠ses en nuestro conjunto de datos.\n\n\n\nDesempleo mundial en 2021\n¬°Es hora de explorar algunos de los datos num√©ricos en unemployment! ¬øCu√°l fue el desempleo t√≠pico en un a√±o determinado? ¬øCu√°l era la tasa de desempleo m√≠nima y m√°xima, y c√≥mo era la distribuci√≥n de las tasas de desempleo en el mundo? Un histogrpama es una buena forma de hacerse una idea de las respuestas a estas preguntas.\nTu tarea en este ejercicio es crear un histograma que muestre la distribuci√≥n de las tasas de paro mundiales en 2021.\n\nInstrucciones\n\nImporta las bibliotecas de visualizaci√≥n necesarias\nCrea un histograma de la distribuci√≥n de los porcentajes de desempleo de 2021 en todos los pa√≠ses en unemployment; muestra un punto pocentual completo en cada casilla.\n\n\n# Import the required visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of 2021 unemployment; show a full percent in each bin\nsns.histplot(x='2021', data=unemployment, binwidth=1)\nplt.show()\n\n\n\n\n\n\n\n\nParece que el desempleo en el 2021 se mantuvo alrededor del 3% al 8% para la mayor√≠a de los pa√≠ses en el conjunto de datos, pero algunos pa√≠ses experimentaron un desempleo muy alto del 20% al 35%.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#validaci√≥n-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#validaci√≥n-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Validaci√≥n de datos",
    "text": "Validaci√≥n de datos\n\nValidando los tipos de datos\n\n\nbooks.dtypes\n\n\n\nActualizando los tipos de datos\n\n\nbooks['year'] = books['year'].astype(int)\nbooks.dtypes\n\n\n\n\n\nTipo\nNombre Python\n\n\n\n\nString\nstr\n\n\nInteger\nint\n\n\nFloat\nfloat\n\n\nDictionary\ndict\n\n\nList\nlist\n\n\nBoolean\nbool\n\n\n\n\nValidando datos categ√≥ricos\n\n\nbooks['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara validar los datos que no est√°n en la lista\n\n~books['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara filtrar el DataFrame por los valores en nuestra lista\n\nbooks[books['genre'].isin(['Fiction', 'Non Fiction'])].head()\n\n\n\nValidando los datos num√©ricos\n\nPara ver solo las columnas num√©ricas en un DataFrame:\n\nbooks.select_dtypes('number').head()\n\nPara conocer un intervalo espec√≠fico:\n\nbooks['year'].min()\n\n\n\nbooks['year'].max()\n\n\nSe puede ver una imagen m√°s detallada de la distribuci√≥n de los datos, utilizando boxplot:\n\nsns.boxplot(data=books, x='year')\nplt.show()\n\n\nTambi√©n se puede ver los datos agrupados por una variable categ√≥rica.\n\nsns.boxplot(data=books, x='year', y='genre')\nplt.show()\n\n\n\nDetectar tipos de datos\n¬°Se ha modificado una columna en el DataFrame unemployment y ahora tiene un tipo de datos incorrecto! Este tipo de datos te impedir√° realizar una exploraci√≥n y un an√°lisis eficaces, por lo que tu tarea consiste en identificar qu√© columna tiene un tipo de datos incorrecto y, a continuaci√≥n, corregirlo.\n\nInstrucciones\nPregunta\n\n¬øCu√°l de las siguientes columnas requiere una actualizaci√≥n de su tipo de datos?\n\n\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019             object\n2020            float64\n2021            float64\ndtype: object\n\n\nRespuestas posibles\n\ncountry_name\ncontinent\n2019\n2021\n\n\n\n\n\nActualiza el tipo de datos de la columna 2019 de unemployment a float.\n¬°Vuelve a imprimir el dtypes del DataFrame umemployment para comprobar que se ha actualizado el tipo de datos!\n\n\n# Update the data type of the 2019 column to a float\nunemployment['2019'] = unemployment['2019'].astype('float')\n\n# Print the dtypes to check your work\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019            float64\n2020            float64\n2021            float64\ndtype: object\n\n\nCambiar el tipo de dato de la columna 2019 significa que ahora puedes realizar c√°lculos sobre ella, incluyendo validar su rango.\n\n\n\nValidar continentes\nTu colega te ha informado de que los datos sobre el desempleo de los pa√≠ses de Ocean√≠a no son fiables, y te gustar√≠a identificar y excluir a estos pa√≠ses de tus datos de unemployment. ¬°La funci√≥n .isin() puede ayudarte con eso!\nTu tarea consiste en utilizar isin() para identificar los pa√≠ses que no est√°n en Ocean√≠a. Estos pa√≠ses deber√≠an devolver True mientras que los pa√≠ses de Ocean√≠a deber√°n devolver False. Esto te permitir√° utilizar los resultados de isin() para filtrar r√°pidamente los pa√≠ses de Ocean√≠a utilizando la indexaci√≥n booleana.\n\n\nInstrucciones\n\nDefina una Serie Booleana que describan si cada continent est√° o no fuera de Ocean√≠a; llama a esta Serie not_oceania.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n\nUtiliza la indexaci√≥n booleana para imprimir el DataFrame unemployment sin ninguno de los datos relacionados con los pa√≠ses de Ocean√≠a.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n# Print unemployment without records related  to countries in Oceania\nprint(unemployment[not_oceania])\n\n    country_code          country_name      continent   2010   2011   2012  \\\n0            AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1            AGO                Angola         Africa   9.43   7.36   7.35   \n2            ALB               Albania         Europe  14.09  13.48  13.38   \n3            ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4            ARG             Argentina  South America   7.71   7.18   7.22   \n..           ...                   ...            ...    ...    ...    ...   \n175          VNM               Vietnam           Asia   1.11   1.00   1.03   \n178          YEM           Yemen, Rep.           Asia  12.83  13.23  13.17   \n179          ZAF          South Africa         Africa  24.68  24.64  24.73   \n180          ZMB                Zambia         Africa  13.19  10.55   7.85   \n181          ZWE              Zimbabwe         Africa   5.21   5.37   5.15   \n\n      2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0    11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1     7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2    15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3     2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4     7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n175   1.32   1.26   1.85   1.85   1.87   1.16   2.04   2.39   2.17  \n178  13.27  13.47  13.77  13.43  13.30  13.15  13.06  13.39  13.57  \n179  24.56  24.89  25.15  26.54  27.04  26.91  28.47  29.22  33.56  \n180   8.61   9.36  10.13  10.87  11.63  12.01  12.52  12.85  13.03  \n181   4.98   4.77   4.78   4.79   4.78   4.80   4.83   5.35   5.17  \n\n[174 rows x 15 columns]\n\n\nValidaste datos categ√≥ricos y usaste tu validaci√≥n .isin() para excluir datos en los que no estabas interesado. Filtrar los datos que no necesitas al comienzo de tu proceso de EDA es una excelente manera de organizarte para la exploraci√≥n que est√° por venir.\n\n\nRango de validaci√≥n\nAhora es el momento de validar nuestros datos num√©ricos. En la lecci√≥n anterior vimos, utilizando .describe(), que la mayor tasa de desempleo durante 2021 fue de casi el 34 %, mientras que la m√°s baja estuvo justo por encima de cero.\nTu tarea en este ejercicio es obtener informaci√≥n mucho m√°s detallada sobre el rango de los datos de unemployment utilizando el diagrama de caja de Seaborn, y tambi√©n visualizar√°s el rango de las tasas de desempleo en cada continente para comprender las diferencias de rango geogr√°fico.\n\nInstrucciones\n\nImprime las tasas de desempleo m√≠nima y m√°ximam en este orden, durante 2021.\nCrea un diagrama de caja de las tasas de desempleo de 2021 (en el eje x), desglosadas por continente (en el eje y).\n\n\n# Print the minimum an maximum unemployment rates during 2021\nprint(unemployment['2021'].min(), unemployment['2021'].max())\n\n# Create a boxplot of 2021 unemployment rates, broken down by continent\nsns.boxplot(data=unemployment, x='2021', y='continent', \n            hue='continent', legend=False)\nplt.show()\n\n0.26 33.56\n\n\n\n\n\n\n\n\n\nObserva c√≥mo var√≠an los rangos de desempleo entre continentes. Por ejemplo, el percentil 50 de √Åfrica es m√°s bajo que el de Am√©rica del Norte, pero el rango es mucho m√°s amplio.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Resumen de datos",
    "text": "Resumen de datos\n\nExplorando grupo de datos\n\n.groupby() grupo de datos por categor√≠a.\nFunci√≥n de agregaci√≥n indica c√≥mo se resume un grupo de datos.\n\n\n\nbooks.groupby('genre').mean()\n\n\n\nFunciones de agregaci√≥n\n\nSuma: .sum()\nConteo: .cont\nM√≠nimo: .min()\nM√°ximo: .max()\nVarianza: .var()\nDesviaci√≥n est√°ndar: .std()\n\nAgregaci√≥n de datos no agrupados\n\n.agg() aplica funciones de agregaci√≥n a trav√©s de un DataFrame\nPor defecto agrega todas las filas de una columna determinada\nSe suele utilizar cuando queremos m√°s de una funci√≥n\nSolo lo aplica a las columnas num√©ricas\n\n\n\nbooks.agg(['mean', 'std'])\n\n\n\nEspecificando agregaciones para columnas\n\n\nbooks.agg({'rating': ['mean', 'std'], 'year': ['median']})\n\n\n\nNombrando columnas resumen\n\n\nbooks.groupby('genre').agg(\n    mean_rating = ('rating', 'mean'),\n    std_rating = ('rating', 'std'),\n    median_year = ('year', 'median')\n)\n\n\n\nVisualizando res√∫menes categoricos\n\nCalculan autom√°ticamente la media de una variable cuantitativa\n\n\n\nsns.barplot(data=books, x='genre', y='rating')\nplt.show()\n\n\n\nRes√∫menes con .groupby() y .agg()\nEn este ejercicio, explorar√°s las medias y desviaciones est√°ndar de los datos anuales de desempleo. En primer lugar, encontrar√°s las medias y desviaciones est√°ndar independientemente del continente para observar las tendencias mundiales del desempleo. Despu√©s, comprobar√°s las tendencias del desempleo desglosadas por continente.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nInstrucciones\n\nImprime la media y las desviaci√≥n est√°ndar d elas tasas de paro de cada a√±o (en ese orden).\n\n\n# Print the mean and standard deviation of rates by year\nprint(unemployment[\n    ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].agg(['mean', 'std']))\n\n          2010      2011      2012      2013      2014      2015      2016  \\\nmean  8.409286  8.315440  8.317967  8.344780  8.179670  8.058901  7.925879   \nstd   6.248887  6.266795  6.367270  6.416041  6.284241  6.161170  6.045439   \n\n          2017      2018      2019      2020  \nmean  7.668626  7.426429  7.243736  8.420934  \nstd   5.902152  5.818915  5.696573  6.040915  \n\n\n\nImprime la media y la desviaci√≥n est√°ndar (en ese orden) de las tasas de paro de cada a√±o agrupadas por continente.\n\n\n# Print yearly mean and standard deviation grouped by continent\nprint(unemployment[\n    ['continent', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].groupby(\"continent\").agg(['mean', 'std']))\n\n                    2010                 2011                 2012            \\\n                    mean       std       mean       std       mean       std   \ncontinent                                                                      \nAfrica          9.343585  7.411259   9.369245  7.401556   9.240755  7.264542   \nAsia            6.240638  5.146175   5.942128  4.779575   5.835319  4.756904   \nEurope         11.008205  6.392063  10.947949  6.539538  11.325641  7.003527   \nNorth America   8.663333  5.115805   8.563333  5.377041   8.448889  5.495819   \nOceania         3.622500  2.054721   3.647500  2.008466   4.103750  2.723118   \nSouth America   6.870833  2.807058   6.518333  2.801577   6.410833  2.936508   \n\n                    2013                 2014            ...      2016  \\\n                    mean       std       mean       std  ...      mean   \ncontinent                                                ...             \nAfrica          9.132453  7.309285   9.121321  7.291359  ...  9.277547   \nAsia            5.852128  4.668405   5.853191  4.681301  ...  6.094894   \nEurope         11.466667  6.969209  10.971282  6.759765  ...  9.394615   \nNorth America   8.840556  6.081829   8.512222  5.801927  ...  7.941111   \nOceania         3.980000  2.640119   3.976250  2.659205  ...  3.877500   \nSouth America   6.335000  2.808780   6.347500  2.834332  ...  7.230833   \n\n                             2017                2018                2019  \\\n                    std      mean       std      mean       std      mean   \ncontinent                                                                   \nAfrica         7.459439  9.284528  7.407620  9.237925  7.358425  9.264340   \nAsia           5.051796  6.171277  5.277201  6.090213  5.409128  5.949149   \nEurope         5.822793  8.359744  5.177845  7.427436  4.738206  6.764359   \nNorth America  5.503090  7.391111  5.326446  7.281111  5.253180  7.095000   \nOceania        2.477866  3.872500  2.492834  3.851250  2.455893  3.773750   \nSouth America  3.052309  7.281667  3.398994  7.496667  3.408856  7.719167   \n\n                              2020            \n                    std       mean       std  \ncontinent                                     \nAfrica         7.455293  10.307736  7.928166  \nAsia           5.254008   7.012340  5.699609  \nEurope         4.124734   7.470513  4.071218  \nNorth America  4.770490   9.297778  4.963045  \nOceania        2.369068   4.273750  2.617490  \nSouth America  3.379845  10.275000  3.411263  \n\n[6 rows x 22 columns]\n\n\nEstos datos est√°n bien resumidos, pero es un poco largo. ¬øQu√© pasar√≠asi quisieras enfocarte en un resumen de solo un a√±o y hacerlo m√°s legible? ¬°Int√©ntalo en el siguiente ejercicio!\n\n\n\nAgregaciones con nombre\nYa has visto c√≥mo .groupby() y .agg() pueden combinarse para mostrar res√∫menes para categor√≠as. A veces, es √∫til nombrar nuevas columnas al agregar, para que se quede claro en la salida del c√≥digo qu√© agregaciones se est√°n aplicando y d√≥nde.\nTu tarea consiste en crear un DataFrame llamado continent_summary que muestre una fila por cada continente. Las columnas del DataFram,e contendr√°n la tasa de paro media de cada continente en 2021, as√≠ como la desviaci√≥n est√°ndar de la tasa de empleo del 2021. Y por supuesto, ¬°renombrar√°s las columnas para que su contenido quede claro!\n\nInstrucciones\n\nCrea una columna llamada mean_rate_2021 que muestre la tasa de paro media de 2021 para cada continente.\nCrea una columna llamada std_rate_2021 que muestre la desviaci√≥n est√°ndar de la tasa de paro de 2021 para cada continente.\n\n\ncontinent_sumary = unemployment[\n    ['continent', '2021']\n].groupby('continent').agg(\n    # Create the mean_rate_2021 column\n    mean_rate_2021 = ('2021', 'mean'),\n    # Create the std_rate_2021 column\n    std_rate_2021 = ('2021', 'std'),\n)\nprint(continent_sumary)\n\n               mean_rate_2021  std_rate_2021\ncontinent                                   \nAfrica              10.473585       8.131636\nAsia                 6.906170       5.414745\nEurope               7.414872       3.947825\nNorth America        9.155000       5.076482\nOceania              4.280000       2.671522\nSouth America        9.924167       3.611624\n\n\nEl desempleo promedio de 2021 vari√≥ ampliamente por continente, y tambi√©n lo hizo el desempleo dentro de esos continentes.\n\n\n\nVisualizar res√∫menes categ√≥ricos\nComo has aprendido en este cap√≠tulo, Seaborn tiene muchas visualizaciones estupendas para la exploraci√≥n, incluido un gr√°fico de barras para mostrar un valor medio agregado por categor√≠a de datos.\nEn Seaborn, los gr√°ficos de barras incluyen una barra vertical que indica el intervalo de confianza del 95 % para la media categ√≥rica. Como los intervalos de confianza se calculan utilizando tanto el n√∫mero de valores como la variabilidad de esos valores, dan una indicaci√≥n √∫til de hasta qu√© punto se puede confiar en los datos.\nTu tarea consiste en crear un diagrama de barras para visualizar las medias y los intervalos de confianza de las tasas de desempleo en los distintos continentes.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nCrea un diagrama de barras que muestre los continentes en el eje x y lsus respectivas tasas medias de desempleo en 2021 en el eje y.\n\n\n# Create a bar plot of continents and their average unemployment\nsns.barplot(data=unemployment, x='continent', y='2021',\n            hue='continent', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\nAunque Europa tiene un mayor desempleo promedio que Asia, tambi√©n tiene un intervalo de confianza m√°s peque√±o para ese promedio, por lo que el valor promedio es m√°s confiable.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "index.html#descripci√≥n",
    "href": "index.html#descripci√≥n",
    "title": "An√°lisis Exploratorio de Datos en Python",
    "section": "Descripci√≥n",
    "text": "Descripci√≥n\nAs√≠ que tienes algunos datos interesantes, ¬øpor d√≥nde empiezas tu an√°lisis? Este curso cubrir√° el proceso de exploraci√≥n y an√°lisis de datos, desde la comprensi√≥n de lo que se incluye en un conjunto de datos hasta la incorporaci√≥n de los resultados de la exploraci√≥n a un flujo de trabajo de ciencia de datos.\nUtilizando datos sobre cifras de desempleo y precios de billetes de avi√≥n, aprovechar√°s Python para resumir y validar datos, calcular, identificar y reemplazar valores perdidos, y limpiar valores num√©ricos y categ√≥ricos. A lo largo del curso, crear√°s hermosas visualizaciones Seaborn para comprender las variables y sus relaciones.\nPor ejemplo, examinar√°s c√≥mo se relacionan el consumo de alcohol y el rendimiento de los alumnos. Por √∫ltimo, el curso mostrar√° c√≥mo los hallazgos exploratorios alimentan los flujos de trabajo de la ciencia de datos creando nuevas caracter√≠sticas, equilibrando caracter√≠sticas categ√≥ricas y generando hip√≥tesis a partir de los hallazgos.\nAl final de este curso, tendr√°s la confianza necesaria para realizar tu propio an√°lisis exploratorio de datos (EDA) en Python. ¬°Ser√°s capaz de explicar tus conclusiones visualmente a los dem√°s y sugerir los siguientes pasos para recopilar informaci√≥n a partir de tus datos!",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html",
    "href": "02_Limpieza_e_imputacion_de_datos.html",
    "title": "Limpieza e imputaci√≥n de datos",
    "section": "",
    "text": "Tratar los datos que faltan\nExplorar y analizar datos a menudo significa tratar con valores perdidos, tipos de datos incorrectos y valores at√≠picos. En este cap√≠tulo, aprender√°s t√©cnicas para gestionar estos problemas y agilizar tus procesos en EDA.\nprint(salaries.isna().sum())\nthreshold = len(salaries) * 0.05\nprint(threhold)\ncols_to_drop = salaries.columns[salaries.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\nsalaries.dropna(subset=cols_to_drop, inplace=True) # Para actualizar el DataFrame\ncols_with_missing_values = salaries.columns[salaries.isna().sum() &gt; 0]\nprint(cols_with_missing_values)\nfor col in cols_with_missing_values[:-1]:\n        salaries[col].fillna(salaries[col].mode()[0])\nprint(salaries.isna().sum())\nsalaries_dict = salaries.groupby('Experience')['Salary_USD'].median().to_dict()\nprint(salaries_dict)\nsalaries['Salary_USD'] = salaries['Salary_USD'].fillna(salaries['Experience'].map(salaries_dict))",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Limpieza e imputaci√≥n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categ√≥ricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categ√≥ricos",
    "title": "Limpieza e imputaci√≥n de datos",
    "section": "Convertir y analizar datos categ√≥ricos",
    "text": "Convertir y analizar datos categ√≥ricos\n\nPrevisualizar los datos\n\n\nprint(salaries.select_dtypes('object').head())\n\n\n\nT√≠tulos de los trabajos\n\n\nprint(salaries['Designation'].value_counts())\n\n\n\nprint(salaries['Designation'].nunique())\n\n\n\n\nExtrayendo valores desde las categor√≠as\n\nEl formato actual de los datos limita la capacidad de generar informaci√≥n.\npandas.Series.str.contains()\n\nBusca en una columna una cadena especifica o m√∫ltiples cadenas.\n\n\n\n\nsalaries['Designation'].str.contains('Scientist')\n\n\n\nFiltrar filas que contienen una o m√°s frases\n\nPalabras de interes: Machine Learning o AI\n\n\n\nsalaries['Designation'].str.contains('Machine Learning|AI')\n\n\n\nBuscar m√∫ltiples frases en una cadena de caracteres\n\nPalabras de interes: Cualquiera que inicie con Data\n\n\n\nsalaries['Designation'].str.contains('ÀÜData')\n\n\nAhora que se tiene una idea de c√≥mo funciona este m√©todo, definamos una lista de t√≠tulos de trabajo que queremos encontrar:\n\njob_categories = ['Data Science', 'Data Analytics',\n                   'Data Engineering', 'Machine Learning',\n                   'Managerial', 'Consultant']\n\nLuego necesitamos crear variables que contengan nuestros filtros\n\ndata_science = 'Data Scientist|NLP'\ndata_analyst = 'Analyst|Analytics'\ndata_engineer = 'Data Engineer|ETL|Architect|Infrastructure'\nml_engineer = 'Machine Learning|ML|Bid Data|AI'\nmanager = 'Manager|Head|Director|Lead|Principal|Staff'\nconsultant = 'Consultant|Freelance'\n\nEl siguiente paso es crear una lista con nuestro rango de condiciones para el m√©todo str.contains\n\nconditions = [\n    (salaries['Designation'].str.contains(data_science)),\n    (salaries['Designation'].str.contains(data_analyst)),\n    (salaries['Designation'].str.contains(data_engineer)),\n    (salaries['Designation'].str.contains(ml_engineer)),\n    (salaries['Designation'].str.contains(manager)),\n    (salaries['Designation'].str.contains(consultant))\n]\n\nFinalmente, podemos crear nuestra nueva columna Job_Category usando la funci√≥n de selecci√≥n de Numpy\n\nsalaries['Job_Category'] = np.select(conditions,\n                                     job_categories,\n                                     default='Other')\n\nAl obtener una vista previa de la Designaci√≥n y nuestra nueva columna Job_Category, podemos verificar los primeros cinco valores.\n\nprint(salaries[['Designation', 'Job_Category']].head())\n\n\n\nVisualizaci√≥n de la frecuencia de la categor√≠a job\n\n\nsns.countplot(data=salaries, x='Job_Category')\nplt.show()\n\n\n\nEncontrar el n√∫mero de valores √∫nicos\nTe gustar√≠a practicar algunas de las habilidades de manipulaci√≥n y an√°lisis de datos categ√≥ricos que acabas de ver. Para ayudarte a identificar qu√© datos podr√≠an reformatearse para extraer valor, vas a averiguar qu√© columnas no num√©ricas del conjunto de datos planes tienen un gran n√∫mero de valores √∫nicos.\n\nInstrucciones\n\nFiltra planes para las columnas que sean del tipo datos \"object\".\nRecorre las columnas del conjunto de datos.\nA√±ade el iterador de columna a la sentencia print y, a continucaci√≥n, llama a la funci√≥n para que devuelva el n√∫mero de valores √∫nicos de la columna.\n\n\n# Filter the DataFrame for objects columns\nnon_numeric = planes.select_dtypes('object')\n\n# Loop through columns\nfor column in non_numeric.columns:\n    # Print the number of unique values\n    print(f\"Number of unique values in {column} column: {non_numeric[column].nunique()}\")\n\nNumber of unique values in Airline column: 8\nNumber of unique values in Date_of_Journey column: 44\nNumber of unique values in Source column: 5\nNumber of unique values in Destination column: 6\nNumber of unique values in Route column: 122\nNumber of unique values in Dep_Time column: 218\nNumber of unique values in Arrival_Time column: 1220\nNumber of unique values in Duration column: 362\nNumber of unique values in Total_Stops column: 5\n\n\nCuriosamente, \"Duration\" es actualmente una columna de tipo objeto cuando deber√≠a ser una columna num√©rica, ¬°y tiene 362 valores √∫nicos! Vamos a averiguar m√°s sobre esta columna.\n\n\n\nCategor√≠a de duraci√≥n de vuelos\nComo has visto, hay 362 valores √∫nicos en la columna \"Duration\" de planes. Llamando a planes['Duration'].head(), vemos los siguientes valores.\n\n\n0        19h\n1     5h 25m\n2     4h 45m\n3     2h 25m\n4    15h 30m\nName: Duration, dtype: object\n\n\nParece que no ser√° sencillo convertirlo a n√∫meros. Sin embargo, ¬°podr√≠as clasificar los vuelos por duraci√≥n y examinar la frecuencia de las distintas longitudes de vuelo!\nCrear√°s una columna \"Duration_Category\" en el DataFrame planes. Antes tendr√°s que crear una lista de valores que deseas insertar en el DataFrame, seguida de los valores existentes a partir de los cuales deben crearse.\n\nInstrucciones\n\nCrea una lista de categor√≠as que contengan \"Short-haul\", \"Medium\" y \"Long-haul\".\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n\n\n\n\nCrea short_flights, una cadena para capturar valores de \"0h\", \"1h\", \"2h\", \"3h\", \"4h\" teniendo cuidado de evitar valores como \"10h\".\nCrea medium_flights para capturar cualquier valor entre cinco y nueve horas. ~\nCrea long_flights para capturar cualquier valor comprendido entre 10 y 16 horas, ambos inclusive.\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n# Create short-haul values\nshort_flights = '^0h|^1h|^2h|^3h|^4h'\n\n# Create medium-haul values\nmedium_flights = '^5h|^6h|^7h|^8h|^9h'\n\n# Create long-haul values\nlong_flights = '^10h|^11h|^12h|^13h|^14h|^15h|^16h'\n\nAhora has creado tus categor√≠as y valores, es hora de agregar condicionalmente las categor√≠as en el DataFrame\n\n\n\nA√±adir categor√≠as de duraci√≥n\nAhora que has configurado las categor√≠as y los valores que quieres capturar, ¬°es hora de construir una nueva columna para analizar la frecuencia de los vuelos seg√∫n su duraci√≥n!\nLas variablesflight_categories, short_flights, medium_flights y long_flights que creaste anteriormente est√°n a tu disposici√≥n.\n\nimport numpy as np\n\n\nInstrucciones\n\nCrea conditions, una lista que contenga subconjuntos de planes['Duration'] basados en short_flights, medium_flights y long_flights.\nCrea la columna \"Duration_Category\" llamando a una funci√≥n que acepte tu lista conditions y flight_categories, estableciendo los valores no encontrados en \"Extreme duration\".\nCrea un gr√°fico fque muestre el recuento de cada categor√≠a.\n\n\n# Create conditions for values in flight_categories to be created\nconditions = [\n    (planes['Duration'].str.contains(short_flights)),\n    (planes['Duration'].str.contains(medium_flights)),\n    (planes['Duration'].str.contains(long_flights))\n]\n\n# Apply the conditions list to the flight_categories\nplanes['Duration_Category'] = np.select(conditions, flight_categories,\n                                        default='Extreme duration')\n\n# Plot the counts of each category√ü\nsns.countplot(data=planes, x='Duration_Category',\n              hue='Duration_Category', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\n¬°Est√° claro que la mayor√≠a de los vuelos son de corta distancia.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Limpieza e imputaci√≥n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-num√©ricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-num√©ricos",
    "title": "Limpieza e imputaci√≥n de datos",
    "section": "Trabajar con datos num√©ricos",
    "text": "Trabajar con datos num√©ricos\n\nEl dataset origina de los salarios\n\n\nprint(salaries.info())\n\n\n\nSalario en Rupias\n\n\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nConvirtiendo cadena de caracteres en n√∫meros\n\nRemover las comas de los valores Salary_In_Rupees\nConvertir la columna a tipo de dato float\nCrear una nueva columna convirtiendo la moneda a d√≥lares\n\n\n\npd.Series.str.replace('Caracter a remover', 'Caracter a reemplazar')\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].str.replace(',', '')\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].astype(float) \n\n1 Indian Rupee = 0.012 US Dollars\n\nsalaries['Salary_USD'] = salaries['Salary_In_Rupees'] * 0.012\n\n\nPrevisualizando la nueva columna\n\n\nprint(salaries[['Salary_In_Rupees', 'Salary_USD']].head())\n\n\n\nA√±adiendo un resumen esrtad√≠stico al DataFrame\n\n\nsalaries.groupby('Company_Size')['Salary_USD'].mean()\n\n\nCalculo de la desviaci√≥n est√°ndar de los salarios por experiencia:\n\n\nsalaries['std_dev'] = salaries.groupby('Experience') \\ \n                      ['Salary_USD'].transform(lambda x: x.std())\n\n\nprint(salaries[['Experience', 'std_dev']].value_counts())\n\n\nRepitamos el proceso para otros datos estad√≠sticos:\n\nsalaries['median_by_comp_size'] = salaries.groupby('Company_Size') \\\n                                  ['Salary_USD'].transform(lambda x: x.median())\n\n\nprint(salaries[['Company_Size', 'median_by_comp_size']].head())\n\n\n\nDuraci√≥n del vuelo\nTe gustar√≠a analizar la duraci√≥n de los vuelos, pero por desgracia, la columna \"Duration\" de DataFrame planes contiene actualmente valores de cadena.\nTendr√°s que limpiar la columna y convertirla al tipo de datos correcto para el an√°lisis.\n\nimport re\n\ndef duration_to_decimal_str(duration_str: str) -&gt; str:\n    '''\n    Convierte una duraci√≥n de vuelo de formato '2h 30m' a una cadena en formato decimal en horas, como '2.5h'.\n    \n    Par√°metros:\n    -----------\n    duration_str : str\n        Cadena de texto que representa la duraci√≥n de un vuelo, como '2h 30m', '45m', '19h', etc.\n\n    Retorna:\n    --------\n    str\n        Cadena con duraci√≥n expresada en horas decimales, con un solo decimal y el sufijo 'h'. Ej: '2.5h'\n    '''\n    horas = re.search(r'(\\d+)\\s*h', duration_str)\n    minutos = re.search(r'(\\d+)\\s*m', duration_str)\n\n    h = int(horas.group(1)) if horas else 0\n    m = int(minutos.group(1)) if minutos else 0\n\n    decimal_hours = round(h + m / 60, 1)\n    return f'{decimal_hours}h'\n\n\n\nplanes['Duration'] = planes['Duration'].apply(duration_to_decimal_str)\n\n\nInstrucciones\n\nImprime los cinco primeros valores de la columna \"Duration\".\n\n\n# Preview the column\nprint(planes['Duration'].head())\n\n0    19.0h\n1     5.4h\n2     4.8h\n3     2.4h\n4    15.5h\nName: Duration, dtype: object\n\n\n\nRetira \"h\" de la columna\n\n\n# Remove the string character\nplanes['Duration'] = planes['Duration'].str.replace('h', '')\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: object\n\n\n\nConvierte la columna al tipo de datos float.\n\n\n# Convert to float data type\nplanes['Duration'] = planes['Duration'].astype(float)\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: float64\n\n\n\nTraza un histograma de los valores de \"Duration\"\n\n\n# Plot a histogram\nsns.histplot(data=planes, x='Duration')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA√±adir estad√≠sticas descriptivas\nAhora \"Duration\" y \"Price\"contienen valores num√©ricos en el DataFrame planes, y te gustar√≠a calcular para ellos estad√≠sticas de resumen condicionadas a los valores de otras columnas.\n\nInstrucciones\n\nA√±ade una columna a planes que contenga la desviaci√≥n est√°ndar de \"Price\" basada en \"Airline\".\n\n\n# Price standard deviation by Airline\nplanes['airline_price_st_dev'] = planes.groupby('Airline')['Price'].transform(lambda x: x.std())\nprint(planes[['Airline', 'airline_price_st_dev']].value_counts())\n\nAirline            airline_price_st_dev\nJet Airways        4159.846432             3082\nIndiGo             2245.529140             1632\nAir India          3692.609285             1399\nMultiple carriers  3558.323763              959\nSpiceJet           1798.900648              653\nVistara            2888.915498              376\nAir Asia           1979.826234              260\nGoAir              2764.926625              147\nName: count, dtype: int64\n\n\n\nCalcula la mediana de \"Duration\" en \"Airline\", almacen√°ndola como una columna llamada \"airline_median_duration\".\n\n\n# Median Duration by Airline\nplanes['airline_median_duration'] = planes.groupby('Airline')['Duration'].transform(lambda x: x.median())\nprint(planes[['Airline', 'airline_median_duration']].value_counts())\n\nAirline            airline_median_duration\nJet Airways        13.3                       3082\nIndiGo             2.9                        1632\nAir India          15.5                       1399\nMultiple carriers  10.2                        959\nSpiceJet           2.5                         653\nVistara            3.2                         376\nAir Asia           2.8                         260\nGoAir              2.9                         147\nName: count, dtype: int64\n\n\n\nEncuenta la media \"Price\" por \"Destination\", guard√°ndola como una columna llamada \"price_destination_mean\".\n\n\n# Mean Price by Destination\nplanes['price_destination_mean'] = planes.groupby('Destination')['Price'].transform(lambda x: x.mean())\nprint(planes[['Destination', 'price_destination_mean']].value_counts())\n\nDestination  price_destination_mean\nCochin       10473.585927              3631\nBanglore     9093.622872               2291\nDelhi        5248.541082                998\nNew Delhi    11579.306944               720\nHyderabad    5190.274021                562\nKolkata      4907.156863                306\nName: count, dtype: int64\n\n\nParece que Jet Airways tiene la mayor desviaci√≥n est√°ndar en precio, Air India tiene la mayor duraci√≥n median, y Nueva Delhi, en promedio, es el destiono m√°s caro. Ahora veamos c√≥mo manejar los datos at√≠picos.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Limpieza e imputaci√≥n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#gesti√≥n-de-valores-at√≠picos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#gesti√≥n-de-valores-at√≠picos",
    "title": "Limpieza e imputaci√≥n de datos",
    "section": "Gesti√≥n de valores at√≠picos",
    "text": "Gesti√≥n de valores at√≠picos\n\nQu√© es un outlier?\n\nEs una observaci√≥n que est√° muy alejada de otros puntos de datos.\n\nUsando estad√≠stica descriptiva\n\n\nprint(salaries['Salary_USD'].describe())\n\n\n\nUsando el rango intercuartil\n\nRango intercuartil (IQR)\n\nIQR = 75th - 25th percentil\nUpper outliers &gt; 75th percentile + (1.5 * IQR)\nLower Outliers &lt; 25th percentile - (1.5 * IQR)\n\n\n\n\nsns.boxplot(data=salaries, y='Salaary_USD')\nplt.show()\n\n\n\nIdentificando Umbrales\n\n\n# 75th percentil\nseventy_fifth = salaries['Salary_USD'].quantile(0.75)\n\n# 25th percentil\ntwenty_fifth = salaries['Salary_USD'].quantile(0.25)\n\n# Interquartil range\nsalaries_iqr = seventy_fifth - twenty_fifth\n\nprint(salaries_iqr)\n\n\n\n# Upper threshold\nupper = seventy_fifth + (1.5 * salaries_iqr)\n\n# Lower threshold\nlower = twenty_fifth - (1.5 * salaries_iqr)\n\nprint(upper, lower)\n\n\n\nSubdividiendo nuestros datos\n\n\nsalaries[(salaries['Salary_USD'] &lt; lower) | (salaries['Salary_USD'] &gt; upper)] \\\n        [['Experience', 'Employee_Location', 'Salary_USD']]\n\n\n\n¬ø Por qu√© buscar los Outliers?\n\nLos Outliers son valores extremos\n\nPueden no representar con precisi√≥n los datos\n\nPueden sesgar la media y la desviaci√≥n est√°ndar\nPruebas de estad√≠stica y modelos de machine learning requieren datos que tengan una distribuci√≥n normal y no esten sesgados.\n\nQu√© hacer con los Outliers?\n\nPreguntas que nos debemos hacer:\n\nPor qu√© existen los outliers?\nLos valores son precisos?\n\n\nEliminaci√≥n de Outliers\n\n\nno_outliers = salaries[(salaries['Salar_USD'] &gt; lower) & (salaries['Salary_USD'] &lt; upper)]\n\n\nprint(no_outliers['Salary_USD'].describe())\n\n\n\nDistribuci√≥n de Salarios\n\n\n\nQu√© hacer con los valores at√≠picos?\nIdentificar los valores at√≠picos es un paso integral en la realizaci√≥n de an√°lisis exploratorios de datos.\nEn este ejercicio, se te presentar√°n escenarios en los que hay valores at√≠picos, y tendr√°s que decidir qu√© acci√≥n debes tomar.\n\nInstrucciones\n\nColoca cada escenario en el cubo adecuado en funci√≥n del enfoque que deba adaptarse para tratar los valores at√≠picos.\n\n\n\n\n\n\n\nElimina los valores at√≠picos\nDejar los valores at√≠picos en el conjunto de datos\n\n\n\n\nUn sensor de temperatura tiene un registro de 100 grados Celsius, pero el sensor solo funciona correctamente a temperaturas de hasta 80 grados.\nse registran las alturas de distintos animales y uno de ellos es m√°s de 1.5 veces la IQR m√°s el percentil 75.\n\n\nLa velocidad de un coche se registra como 5000 km/h.\nLos pa√≠ses tienen una superficie total media de 667.143 km2, pero un pa√≠s tiene 1.637.687 km2.\n\n\nUn participante en un estudio tiene una edad de menos 35 a√±os.\nUn jugador de baloncesto hace una media de 35 puntos por partido cuando la media en toda la liga es de solo 10 puntos por partido.\n\n\n\n\nPuede ser dif√≠cil decidir qu√© hacer con los valores at√≠picos, pero debes saber c√≥mo gestionarlos, ¬°ya que a menudo se dan en el mundo real!\n\n\n\nIdentificar valores at√≠picos\nHas demostrado que reconoces qu√© hacer cuando se te presentan valores at√≠picos, pero ¬øPuedes identificarlos utilizando visualizaciones?\nIntenta averiguar si hay valores at√≠picos en las columnas \"Price\" o \"Duration\" del dataframe planes.\n\nIntrucciones\n\nTraza la distribuci√≥n de la columna \"Price\" de planes.\n\n\n# Plot a histogram of flight prices\nsns.histplot(data=planes, x='Price')\nplt.show()\n\n\n\n\n\n\n\n\n\nMuestra las estad√≠sticas descriptivas de la duraci√≥n del vuelo.\n\n\n# Display descriptive statistics for flight duration\nprint(planes['Duration'].describe())\n\ncount    8508.000000\nmean       10.726704\nstd         8.472415\nmin         0.100000\n25%         2.800000\n50%         8.700000\n75%        15.500000\nmax        47.700000\nName: Duration, dtype: float64\n\n\n\nPregunta\n\n¬øQu√© columna contiene potencialmente valores at√≠picos?\nRespuestas Posibles\n\n\"Price\"\n\"Duration\"\n\"Price\" y \"Duration\"\nNinguna\n\nLos histogramas, diagramas de caja y estad√≠sticas descriptivas tambi√©n son m√©todos √∫tiles para identificar valores extremos. ¬°Ahora vamos a tratarlos!\n\n\n\nEliminar valores at√≠picos\nAunque eliminar los valores at√≠picos no siempre es el camino a seguir, para tu an√°lisis has decidido que solo incluir√°s los vuelos en los que el \"Price\" no sea un valor at√≠pico.\nPor lo tanto tienes que encontrar el umbral superior y utilizarlo para eliminar los valores que lo superen del Dataframe planes.\n\nInstrucciones\n\nHalla los percentiles 75 y 25, guardando como price_seventy_fifth y price_twenty_fifth respectivamente.\n\n\n# Find the 75th and 25th percentiles\nprice_seventy_fifth = planes['Price'].quantile(0.75)\nprice_twenty_fifth = planes['Price'].quantile(0.25)\n\n\nCalcula el IQR, almacen√°ndolo como prices_iqr.\n\n\n# Calculate iqr\nprices_iqr = price_seventy_fifth - price_twenty_fifth\nprint(prices_iqr)\n\n7014.0\n\n\n\nCalcula los umbrales superior e inferior de los valores at√≠picos.\n\n\n# Calculate the thresholds\nupper = price_seventy_fifth + (1.5 * prices_iqr)\nlower = price_twenty_fifth - (1.5 * prices_iqr)\n\n\nElimina los valores at√≠picos de planes.\n\n\n# Subset the data\nplanes = planes[(planes['Price'] &gt; lower) & (planes['Price'] &lt; upper)]\n\nprint(planes['Price'].describe())\n\ncount     8438.000000\nmean      8877.466046\nstd       4001.838236\nmin       1759.000000\n25%       5224.000000\n50%       8372.000000\n75%      12121.000000\nmax      22270.000000\nName: Price, dtype: float64\n\n\n¬°Habilidades rid√≠culas para eliminar valores at√≠picos! Lograste crear umbrales basados en el IQR y los usaste para filtrar el conjunto de datos planes para eliminar precios extremos. Originalmente, el conjunto de datos ten√≠a un precio m√°ximo de casi 55000, pero la salida de planes['Price'].describe() muestra que el m√°ximo se ha reducido a alrededor de 23000, ¬°reflejando una distribuci√≥n menos sesgada para el an√°lisis!",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Limpieza e imputaci√≥n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "href": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "title": "Limpieza e imputaci√≥n de datos",
    "section": "",
    "text": "Por qu√© un dato faltante es un problema?\n\nAfectan las distribuciones\nLos datos de la poblaci√≥n son menos repreesntativos\nPuede resultar en conclusiones incorrectas\n\nEjemplo datos de profesionales de datos\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nData type\n\n\n\n\nWorking_Year\nYear the data was obtained\nFloat\n\n\nDesignation\nJob title\nString\n\n\nExperience\nExperience level e.g., \"Mid\", \"Senior\"\nString\n\n\nEmployment_Satus\nType of employment contract e.g., \"FT\", \"PT\"\nString\n\n\nEmployee_Location\nCountry of employment\nString\n\n\nCompany_Size\nLabels for company size e.g., \"S\", \"M\", \"L\"\nString\n\n\nRemote_Working_Ratio\nPorcentage of time working remotely\nInteger\n\n\nSalary_USD\nSalary in US dollars\nFloat\n\n\n\n\nRevisando los datos faltantes\n\n\n\n\nEstrategias para el manejo de datos faltantes\n\nEliminar los datos faltantes\n\n5 % o menos del total de valores\n\nImputar la media, mediana o la moda\n\nDepende de la distribuci√≥n y contexto\n\nImputar por sub-grupos\n\nDiferentes niveles de experiencia tienen diferente mediana en el salario\n\n\nEliminando valores faltantes\n\n\n\n\n\n\n\nImputando una estad√≠stica de resumen\n\n\n\n\n\nRevisando los valores faltantes que faltan\n\n\n\n\nImputando por subgrupo\n\n\n\n\n\n\nTratar los datos que faltan\nEs importante tratar los datos que faltan antes de empezar el an√°lisis.\nUn enfoque consiste en descartar los valores que faltan si representan una peque√±a proporci√≥n, normalmente el 5 %, de los datos.\nTrabajando con un conjunto de datos sobre precios de tiquetes de avi√≥n, almacenado como un DataFrame de pandas llamado planes, tendr√°s que contar el n√∫mero de valores perdidos en todas las columnas, calcular el cinco porciento de todos los valores, utilizar este umbral para eliminar observaciones y comprobar cu√°ntos valores perdidos quedan en el conjunto de datos.\n\nimport pandas as pd\n\nruta = './data/planes.csv'\nplanes = pd.read_csv(ruta)\nprint(planes.head)\n\n&lt;bound method NDFrame.head of            Airline Date_of_Journey    Source Destination  \\\n0      Jet Airways       9/06/2019     Delhi      Cochin   \n1           IndiGo      12/05/2019   Kolkata    Banglore   \n2           IndiGo      01/03/2019  Banglore   New Delhi   \n3         SpiceJet      24/06/2019   Kolkata    Banglore   \n4      Jet Airways      12/03/2019  Banglore   New Delhi   \n...            ...             ...       ...         ...   \n10655     Air Asia       9/04/2019   Kolkata    Banglore   \n10656    Air India      27/04/2019   Kolkata    Banglore   \n10657  Jet Airways      27/04/2019  Banglore       Delhi   \n10658      Vistara      01/03/2019  Banglore   New Delhi   \n10659    Air India       9/05/2019     Delhi      Cochin   \n\n                       Route Dep_Time  Arrival_Time Duration Total_Stops  \\\n0      DEL ‚Üí LKO ‚Üí BOM ‚Üí COK    09:25  04:25 10 Jun      19h     2 stops   \n1            CCU ‚Üí NAG ‚Üí BLR    18:05         23:30   5h 25m      1 stop   \n2            BLR ‚Üí NAG ‚Üí DEL    16:50         21:35   4h 45m      1 stop   \n3                  CCU ‚Üí BLR    09:00         11:25   2h 25m    non-stop   \n4            BLR ‚Üí BOM ‚Üí DEL    18:55  10:25 13 Mar  15h 30m      1 stop   \n...                      ...      ...           ...      ...         ...   \n10655              CCU ‚Üí BLR    19:55         22:25   2h 30m    non-stop   \n10656              CCU ‚Üí BLR    20:45         23:20   2h 35m    non-stop   \n10657              BLR ‚Üí DEL      NaN         11:20       3h    non-stop   \n10658              BLR ‚Üí DEL    11:30         14:10   2h 40m    non-stop   \n10659  DEL ‚Üí GOI ‚Üí BOM ‚Üí COK    10:55         19:15   8h 20m     2 stops   \n\n                   Additional_Info    Price  \n0                          No info  13882.0  \n1                          No info   6218.0  \n2                          No info  13302.0  \n3                          No info   3873.0  \n4      In-flight meal not included  11087.0  \n...                            ...      ...  \n10655                      No info   4107.0  \n10656                      No info   4145.0  \n10657                          NaN   7229.0  \n10658                      No info  12648.0  \n10659                      No info  11753.0  \n\n[10660 rows x 11 columns]&gt;\n\n\n\nInstrucciones\n\nImprime el n√∫mero de valores perdidos en cada columna del DataFrame\n\n\n# Count the number of missing values in each column\nprint(planes.isna().sum())\n\nAirline            427\nDate_of_Journey    322\nSource             187\nDestination        347\nRoute              256\nDep_Time           260\nArrival_Time       194\nDuration           214\nTotal_Stops        212\nAdditional_Info    589\nPrice              616\ndtype: int64\n\n\n\nCalcula a cu√°ntas observaciones equivale el cinco porciento del DataFrame planes\n\n\n# Find the five percent threshold\nthreshold = len(planes) * 0.05\nprint(threshold)\n\n533.0\n\n\n\n\n\n\nCrea cols_to_drop aplicando una indexaci√≥n booleana a las columnas del DataFrame con valores perdidos menores o iguales que el umbral.\nUtiliza este filtro para eliminar los valores que faltan y guardar el DataFrame actualizado.\n\n\n# Create a filter\ncols_to_drop = planes.columns[planes.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\n\n# Drop missing values for columns below the threshold\nplanes.dropna(subset=cols_to_drop, inplace=True)\n\nprint(planes.isna().sum())\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops'],\n      dtype='object')\nAirline              0\nDate_of_Journey      0\nSource               0\nDestination          0\nRoute                0\nDep_Time             0\nArrival_Time         0\nDuration             0\nTotal_Stops          0\nAdditional_Info    300\nPrice              368\ndtype: int64\n\n\nAl crear un umbral de valores faltantes y usarlo para filtrar columnas, haz logrado eliminar los valores faltantes de todas las columnas excepto \"Additinal_Info\" y \"Price\".\n\n\n\nEstrategias para datos que faltan\nLa regla del cinco porciento ha funcionado muy bien en tu conjunto de dato planes, ¬°eliminando los valores perdidos de nueve de las 11 columnas!\nAhora tienes que decidir qu√© hacer con las columnas \"Additional_Info\" y \"Price\", a las que les faltan los valores 300 y 368 respectivamente.\nPrimero echar√°s un vistazo a lo que contiene \"Additional_Info\", y despu√©s visualizar√°s el precio de los billetes de avi√≥n de distintas compa√±√≠as a√©reas.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nImprime los valores y frecuencias de \"Additional_Info\".\n\n\n# Check the values of the Additional_Info column\nprint(planes['Additional_Info'].value_counts())\n\nAdditional_Info\nNo info                         6399\nIn-flight meal not included     1525\nNo check-in baggage included     258\n1 Long layover                    14\nChange airports                    7\nNo Info                            2\nBusiness class                     1\nRed-eye flight                     1\n2 Long layover                     1\nName: count, dtype: int64\n\n\n\nCrea un boxplot de \"Price\" frente a \"Airline\"\n\n\n# Create a box plot of Price by Airline\nsns.boxplot(data=planes, x='Airline', y='Price',\n            hue='Airline', legend=False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPregunta\n\n\n¬øC√≥mo debes tratar los valores que faltan en \"Additional_Info\" y \"Price\".?\n\nRespuestas Posibles\n\nElimina la columna \"Additional_Info\" e imputa la media para los valores que faltan de \"Price\".\nElimina los valores de \"No info\" de \"Additiona_Info\" e imputa la mediana de los valores que faltan de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la media por \"Airline\" para los valores que falten de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la mediana por \"Airline\" para los valores que falten de \"Price\".\n\nNo necesitamos la columna \"Additional_Info\", y deber√≠as imputar la mediana de \"Price\" por \"Airline\" para representar los datos con precisi√≥n.\n\n\n\nImputar los precios de los aviones que faltan\n!Ahora solo queda una columna con valores perdidos!\nHas eliminado la columna \"Additional_Info\" de planes, el √∫ltimo paso es imputar los datos que faltan en la columna \"Price\" del conjunto de datos.\nComo recordatorio, t√∫ generaste este diagrama de caja, que suger√≠a que imputar el precio medio bas√°ndose en el \"Airline\" ¬°es un enfoque s√≥lido!\n\n# Eliminamos la columna Additional_Info\nplanes = planes.drop('Additional_Info', axis=1)\nplanes.columns\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops', 'Price'],\n      dtype='object')\n\n\n\nInstrucciones\n\nAgrupa planes por aerol√≠nea y calcula el precio medio.\n\n\n# Calculate median plane ticket prices by Airplane\nairline_prices = planes.groupby('Airline')['Price'].median()\n\nprint(airline_prices)\n\nAirline\nAir Asia              5192.0\nAir India             9443.0\nGoAir                 5003.5\nIndiGo                5054.0\nJet Airways          11507.0\nMultiple carriers    10197.0\nSpiceJet              3873.0\nVistara               8028.0\nName: Price, dtype: float64\n\n\n\nConvierte los precios medios agrupados en un diccionario.\n\n\n# Convert to a dictionary\nprices_dict = airline_prices.to_dict()\nprint(prices_dict)\n\n{'Air Asia': 5192.0, 'Air India': 9443.0, 'GoAir': 5003.5, 'IndiGo': 5054.0, 'Jet Airways': 11507.0, 'Multiple carriers': 10197.0, 'SpiceJet': 3873.0, 'Vistara': 8028.0}\n\n\n\n\n\n\nImputa condicionalmente los valores perdidos de \"Price\" asignando los valores de la columna \"Airline\" en funci√≥n de prices_dict\nComprueba si faltan valores\n\n\n# Map the dictionary to missing values of Price by Airline\nplanes['Price'] = planes['Price'].fillna(planes['Airline'].map(prices_dict))\n\n# Check for missing values\nprint(planes.isna().sum())\n\nAirline            0\nDate_of_Journey    0\nSource             0\nDestination        0\nRoute              0\nDep_Time           0\nArrival_Time       0\nDuration           0\nTotal_Stops        0\nPrice              0\ndtype: int64\n\n\nConvertiste un DataFrame agrupado a un diccionario y luego lo usaste para llenar condicionalmente los valores faltantes de \"Price\" bas√°ndote en \"Airline\". Ahora vamos a explorar c√≥mo realizar an√°lisis exploratorio en datos categ√≥ricos.",
    "crumbs": [
      "Cap√≠tulos",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Limpieza e imputaci√≥n de datos</span>"
    ]
  }
]