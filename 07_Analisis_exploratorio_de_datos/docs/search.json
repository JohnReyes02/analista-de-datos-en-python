[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An치lisis Exploratorio de Datos en Python",
    "section": "",
    "text": "Bienvenida\nEste curso interactivo cubre el proceso de exploraci칩n y an치lisis de datos en Python, desde entender un nuevo dataset hasta la limpieza e imputaci칩n de valores.\n游늵 Nivel: Intermedio\n游 Duraci칩n estimada: 4 horas\n游꿘 Incluye c칩digo, visualizaciones y ejercicios",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#m칩dulos-del-curso",
    "href": "index.html#m칩dulos-del-curso",
    "title": "An치lisis Exploratorio de Datos en Python",
    "section": "M칩dulos del curso",
    "text": "M칩dulos del curso\n\nConocer un conjunto de datos\n\nLimpieza e imputaci칩n de datos\nRelaciones en los datos\nConvertir el an치lisis exploratorio en acci칩n",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "index.html#datasets",
    "href": "index.html#datasets",
    "title": "An치lisis Exploratorio de Datos en Python",
    "section": "Datasets",
    "text": "Datasets\nEste curso utiliza los siguientes archivos:\n\nunemployment.csv\ndata_science_salaries.csv\nbooks.csv\ndivorce.csv\nplanes.csv",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html",
    "href": "01_Conocer_un_conjunto_de_datos.html",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "Exploraci칩n inicial\n쮺u치l el la mejor manera de abordar un nuevo conjunto de datos? Aprende a validar y resumir datos categ칩ricos y num칠ricos y a crear visualizaciones Seaborn para comunicar tus conclusiones.\nbooks = pd.read_csv('books.csv')\nbooks.head()\nbooks.info()\nbooks.value_counts('genre')\nbooks.describe()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.histplot(data=books, x='rating')\nplt.show()\nsns.histplot(data=books, x='rating', binwidth=.1)\nplt.show()",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>1</span> <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#exploraci칩n-inicial",
    "href": "01_Conocer_un_conjunto_de_datos.html#exploraci칩n-inicial",
    "title": "Conocer un conjunto de datos",
    "section": "",
    "text": "An치lisis Exploratorio de Datos\n\nEs el proceso de limpiar y revisar datos para:\n\nObterner informaci칩n (Estad칤stica descriptiva, correlaciones)\nGenrar hip칩tesis\n\n\nUna primera mirada con .head()\n\n\n\n\nReuniendo m치s .info()\n\n\n\n\nUna mirada cercana a las columnas categ칩ricas\n\n\n\n\nColumnas num칠ricas con .describe()\n\n\n\n\nVisualizando datos num칠ricos\n\n\n\n\nAjustando la anchura del bin\n\n\n\n\nFunciones para la exploraci칩n inicial\nEst치s investigando las tasas de desempleo en todo el mundo y te han dado un nuevo conjunto de datos con el que trabajar. Los datos se han guardado y cargado para ti como un DataFrame de pandas llamado unemployment. Nunca antes hab칤as visto los datos, as칤 que tu primera tarea es utilizar unas cuantas funciones de pandas para conocer estos nuevos datos.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\n\n\nInstrucciones\n\nUtiliza una funci칩n de pandas para imprimir las cinco primeras filas del DataFrame unemployment.\n\n\n# Print the first five rows of unemployment\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nUtiliza una funci칩n pandas para imprimir un resumen de los valores y tipos de datos de las columnas que no faltan del DataFrame unemployment.\n\n\n# Print a summary of non-missing values and data types in the unemployment DataFrame]\nprint(unemployment.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 182 entries, 0 to 181\nData columns (total 15 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   country_code  182 non-null    object \n 1   country_name  182 non-null    object \n 2   continent     177 non-null    object \n 3   2010          182 non-null    float64\n 4   2011          182 non-null    float64\n 5   2012          182 non-null    float64\n 6   2013          182 non-null    float64\n 7   2014          182 non-null    float64\n 8   2015          182 non-null    float64\n 9   2016          182 non-null    float64\n 10  2017          182 non-null    float64\n 11  2018          182 non-null    float64\n 12  2019          182 non-null    float64\n 13  2020          182 non-null    float64\n 14  2021          182 non-null    float64\ndtypes: float64(12), object(3)\nmemory usage: 21.5+ KB\nNone\n\n\n\nImprime las estad칤sticas de resument (recuento, media, desviaci칩n est치ndar, valores m칤nimo, m치ximo y cuartil) de cada columna num칠rica en unemployment.\n\n\n# Print summary statistics for numerical columns in unemployment\nprint(unemployment.describe())\n\n             2010        2011        2012        2013        2014        2015  \\\ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000   \nmean     8.409286    8.315440    8.317967    8.344780    8.179670    8.058901   \nstd      6.248887    6.266795    6.367270    6.416041    6.284241    6.161170   \nmin      0.450000    0.320000    0.480000    0.250000    0.200000    0.170000   \n25%      4.015000    3.775000    3.742500    3.692500    3.625000    3.662500   \n50%      6.965000    6.805000    6.690000    6.395000    6.450000    6.170000   \n75%     10.957500   11.045000   11.285000   11.310000   10.695000   10.215000   \nmax     32.020000   31.380000   31.020000   29.000000   28.030000   27.690000   \n\n             2016        2017        2018        2019        2020        2021  \ncount  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000  \nmean     7.925879    7.668626    7.426429    7.243736    8.420934    8.390879  \nstd      6.045439    5.902152    5.818915    5.696573    6.040915    6.067192  \nmin      0.150000    0.140000    0.110000    0.100000    0.210000    0.260000  \n25%      3.800000    3.690000    3.625000    3.487500    4.285000    4.335000  \n50%      5.925000    5.650000    5.375000    5.240000    6.695000    6.425000  \n75%     10.245000   10.315000    9.257500    9.445000   11.155000   10.840000  \nmax     26.540000   27.040000   26.910000   28.470000   29.220000   33.560000  \n\n\nAhora haz aprendido que unemployment contiene 182 filas de datos de pa칤ses, incluyendo country_code, country_name, continent y porcentajes de desempleo desde 2010 hasta 2021. 춰Si miraste muy de cerca, podr칤as haber notado que a algunos pa칤ses les falta informaci칩n en la columna continent! Continuemos explorando estos datos en el pr칩ximo ejercicio.\n\n\n\nContar valores categ칩ricos\nRecordemos del ejercicio anterior que el DataFrame unemployment contiene 182 filas de datos de pa칤ses que incluyen country_code, country_name, continent y porcentajes de desempleo de 2010 a 2021.\nAhora vas a explorar los datos categ칩ricos contenidos en unemployment para comprender los datos que contiene relacionados con cada continente.\n\nInstrucciones\n\nUtiliza un m칠todo para contar los valores asociados a cada continent en el DataFrame unemployment.\n\n\n# Count the values associated with each continent in unemployment\nprint(unemployment['continent'].value_counts())\n\ncontinent\nAfrica           53\nAsia             47\nEurope           39\nNorth America    18\nSouth America    12\nOceania           8\nName: count, dtype: int64\n\n\n쯉ab칤as que hay 23 pa칤ses en Am칠rica del Norte, que incluye pa칤ses en el Caribe y Am칠rica Central? Puede que hayas notado que Am칠rica del Norte tiene 18 puntos de datos en el DataFrame unemployment, por lo que nos falta informaci칩n de algunos de los pa칤ses en nuestro conjunto de datos.\n\n\n\nDesempleo mundial en 2021\n춰Es hora de explorar algunos de los datos num칠ricos en unemployment! 쮺u치l fue el desempleo t칤pico en un a침o determinado? 쮺u치l era la tasa de desempleo m칤nima y m치xima, y c칩mo era la distribuci칩n de las tasas de desempleo en el mundo? Un histogrpama es una buena forma de hacerse una idea de las respuestas a estas preguntas.\nTu tarea en este ejercicio es crear un histograma que muestre la distribuci칩n de las tasas de paro mundiales en 2021.\n\nInstrucciones\n\nImporta las bibliotecas de visualizaci칩n necesarias\nCrea un histograma de la distribuci칩n de los porcentajes de desempleo de 2021 en todos los pa칤ses en unemployment; muestra un punto pocentual completo en cada casilla.\n\n\n# Import the required visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create a histogram of 2021 unemployment; show a full percent in each bin\nsns.histplot(x='2021', data=unemployment, binwidth=1)\nplt.show()\n\n\n\n\n\n\n\n\nParece que el desempleo en el 2021 se mantuvo alrededor del 3% al 8% para la mayor칤a de los pa칤ses en el conjunto de datos, pero algunos pa칤ses experimentaron un desempleo muy alto del 20% al 35%.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>1</span> <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#validaci칩n-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#validaci칩n-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Validaci칩n de datos",
    "text": "Validaci칩n de datos\n\nValidando los tipos de datos\n\n\nbooks.dtypes\n\n\n\nActualizando los tipos de datos\n\n\nbooks['year'] = books['year'].astype(int)\nbooks.dtypes\n\n\n\n\n\nTipo\nNombre Python\n\n\n\n\nString\nstr\n\n\nInteger\nint\n\n\nFloat\nfloat\n\n\nDictionary\ndict\n\n\nList\nlist\n\n\nBoolean\nbool\n\n\n\n\nValidando datos categ칩ricos\n\n\nbooks['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara validar los datos que no est치n en la lista\n\n~books['genre'].isin(['Fiction', 'Non Fiction'])\n\n\nPara filtrar el DataFrame por los valores en nuestra lista\n\nbooks[books['genre'].isin(['Fiction', 'Non Fiction'])].head()\n\n\n\nValidando los datos num칠ricos\n\nPara ver solo las columnas num칠ricas en un DataFrame:\n\nbooks.select_dtypes('number').head()\n\nPara conocer un intervalo espec칤fico:\n\nbooks['year'].min()\n\n\n\nbooks['year'].max()\n\n\nSe puede ver una imagen m치s detallada de la distribuci칩n de los datos, utilizando boxplot:\n\nsns.boxplot(data=books, x='year')\nplt.show()\n\n\nTambi칠n se puede ver los datos agrupados por una variable categ칩rica.\n\nsns.boxplot(data=books, x='year', y='genre')\nplt.show()\n\n\n\nDetectar tipos de datos\n춰Se ha modificado una columna en el DataFrame unemployment y ahora tiene un tipo de datos incorrecto! Este tipo de datos te impedir치 realizar una exploraci칩n y un an치lisis eficaces, por lo que tu tarea consiste en identificar qu칠 columna tiene un tipo de datos incorrecto y, a continuaci칩n, corregirlo.\n\nInstrucciones\nPregunta\n\n쮺u치l de las siguientes columnas requiere una actualizaci칩n de su tipo de datos?\n\n\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019             object\n2020            float64\n2021            float64\ndtype: object\n\n\nRespuestas posibles\n\ncountry_name\ncontinent\n2019\n2021\n\n\n\n\n\nActualiza el tipo de datos de la columna 2019 de unemployment a float.\n춰Vuelve a imprimir el dtypes del DataFrame umemployment para comprobar que se ha actualizado el tipo de datos!\n\n\n# Update the data type of the 2019 column to a float\nunemployment['2019'] = unemployment['2019'].astype('float')\n\n# Print the dtypes to check your work\nprint(unemployment.dtypes)\n\ncountry_code     object\ncountry_name     object\ncontinent        object\n2010            float64\n2011            float64\n2012            float64\n2013            float64\n2014            float64\n2015            float64\n2016            float64\n2017            float64\n2018            float64\n2019            float64\n2020            float64\n2021            float64\ndtype: object\n\n\nCambiar el tipo de dato de la columna 2019 significa que ahora puedes realizar c치lculos sobre ella, incluyendo validar su rango.\n\n\n\nValidar continentes\nTu colega te ha informado de que los datos sobre el desempleo de los pa칤ses de Ocean칤a no son fiables, y te gustar칤a identificar y excluir a estos pa칤ses de tus datos de unemployment. 춰La funci칩n .isin() puede ayudarte con eso!\nTu tarea consiste en utilizar isin() para identificar los pa칤ses que no est치n en Ocean칤a. Estos pa칤ses deber칤an devolver True mientras que los pa칤ses de Ocean칤a deber치n devolver False. Esto te permitir치 utilizar los resultados de isin() para filtrar r치pidamente los pa칤ses de Ocean칤a utilizando la indexaci칩n booleana.\n\n\nInstrucciones\n\nDefina una Serie Booleana que describan si cada continent est치 o no fuera de Ocean칤a; llama a esta Serie not_oceania.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n\nUtiliza la indexaci칩n booleana para imprimir el DataFrame unemployment sin ninguno de los datos relacionados con los pa칤ses de Ocean칤a.\n\n\n# Define a Series describing whether each continent is outside of Oceania\nnot_oceania = ~unemployment['continent'].isin(['Oceania'])\n\n# Print unemployment without records related  to countries in Oceania\nprint(unemployment[not_oceania])\n\n    country_code          country_name      continent   2010   2011   2012  \\\n0            AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1            AGO                Angola         Africa   9.43   7.36   7.35   \n2            ALB               Albania         Europe  14.09  13.48  13.38   \n3            ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4            ARG             Argentina  South America   7.71   7.18   7.22   \n..           ...                   ...            ...    ...    ...    ...   \n175          VNM               Vietnam           Asia   1.11   1.00   1.03   \n178          YEM           Yemen, Rep.           Asia  12.83  13.23  13.17   \n179          ZAF          South Africa         Africa  24.68  24.64  24.73   \n180          ZMB                Zambia         Africa  13.19  10.55   7.85   \n181          ZWE              Zimbabwe         Africa   5.21   5.37   5.15   \n\n      2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0    11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1     7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2    15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3     2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4     7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n..     ...    ...    ...    ...    ...    ...    ...    ...    ...  \n175   1.32   1.26   1.85   1.85   1.87   1.16   2.04   2.39   2.17  \n178  13.27  13.47  13.77  13.43  13.30  13.15  13.06  13.39  13.57  \n179  24.56  24.89  25.15  26.54  27.04  26.91  28.47  29.22  33.56  \n180   8.61   9.36  10.13  10.87  11.63  12.01  12.52  12.85  13.03  \n181   4.98   4.77   4.78   4.79   4.78   4.80   4.83   5.35   5.17  \n\n[174 rows x 15 columns]\n\n\nValidaste datos categ칩ricos y usaste tu validaci칩n .isin() para excluir datos en los que no estabas interesado. Filtrar los datos que no necesitas al comienzo de tu proceso de EDA es una excelente manera de organizarte para la exploraci칩n que est치 por venir.\n\n\nRango de validaci칩n\nAhora es el momento de validar nuestros datos num칠ricos. En la lecci칩n anterior vimos, utilizando .describe(), que la mayor tasa de desempleo durante 2021 fue de casi el 34 %, mientras que la m치s baja estuvo justo por encima de cero.\nTu tarea en este ejercicio es obtener informaci칩n mucho m치s detallada sobre el rango de los datos de unemployment utilizando el diagrama de caja de Seaborn, y tambi칠n visualizar치s el rango de las tasas de desempleo en cada continente para comprender las diferencias de rango geogr치fico.\n\nInstrucciones\n\nImprime las tasas de desempleo m칤nima y m치ximam en este orden, durante 2021.\nCrea un diagrama de caja de las tasas de desempleo de 2021 (en el eje x), desglosadas por continente (en el eje y).\n\n\n# Print the minimum an maximum unemployment rates during 2021\nprint(unemployment['2021'].min(), unemployment['2021'].max())\n\n# Create a boxplot of 2021 unemployment rates, broken down by continent\nsns.boxplot(data=unemployment, x='2021', y='continent', \n            hue='continent', legend=False)\nplt.show()\n\n0.26 33.56\n\n\n\n\n\n\n\n\n\nObserva c칩mo var칤an los rangos de desempleo entre continentes. Por ejemplo, el percentil 50 de 츼frica es m치s bajo que el de Am칠rica del Norte, pero el rango es mucho m치s amplio.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>1</span> <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "href": "01_Conocer_un_conjunto_de_datos.html#resumen-de-datos",
    "title": "Conocer un conjunto de datos",
    "section": "Resumen de datos",
    "text": "Resumen de datos\n\nExplorando grupo de datos\n\n.groupby() grupo de datos por categor칤a.\nFunci칩n de agregaci칩n indica c칩mo se resume un grupo de datos.\n\n\n\nbooks.groupby('genre').mean()\n\n\n\nFunciones de agregaci칩n\n\nSuma: .sum()\nConteo: .cont\nM칤nimo: .min()\nM치ximo: .max()\nVarianza: .var()\nDesviaci칩n est치ndar: .std()\n\nAgregaci칩n de datos no agrupados\n\n.agg() aplica funciones de agregaci칩n a trav칠s de un DataFrame\nPor defecto agrega todas las filas de una columna determinada\nSe suele utilizar cuando queremos m치s de una funci칩n\nSolo lo aplica a las columnas num칠ricas\n\n\n\nbooks.agg(['mean', 'std'])\n\n\n\nEspecificando agregaciones para columnas\n\n\nbooks.agg({'rating': ['mean', 'std'], 'year': ['median']})\n\n\n\nNombrando columnas resumen\n\n\nbooks.groupby('genre').agg(\n    mean_rating = ('rating', 'mean'),\n    std_rating = ('rating', 'std'),\n    median_year = ('year', 'median')\n)\n\n\n\nVisualizando res칰menes categoricos\n\nCalculan autom치ticamente la media de una variable cuantitativa\n\n\n\nsns.barplot(data=books, x='genre', y='rating')\nplt.show()\n\n\n\nRes칰menes con .groupby() y .agg()\nEn este ejercicio, explorar치s las medias y desviaciones est치ndar de los datos anuales de desempleo. En primer lugar, encontrar치s las medias y desviaciones est치ndar independientemente del continente para observar las tendencias mundiales del desempleo. Despu칠s, comprobar치s las tendencias del desempleo desglosadas por continente.\n\nimport pandas as pd\n\nruta = './data/clean_unemployment.csv'\nunemployment = pd.read_csv(ruta)\nprint(unemployment.head())\n\n  country_code          country_name      continent   2010   2011   2012  \\\n0          AFG           Afghanistan           Asia  11.35  11.05  11.34   \n1          AGO                Angola         Africa   9.43   7.36   7.35   \n2          ALB               Albania         Europe  14.09  13.48  13.38   \n3          ARE  United Arab Emirates           Asia   2.48   2.30   2.18   \n4          ARG             Argentina  South America   7.71   7.18   7.22   \n\n    2013   2014   2015   2016   2017   2018   2019   2020   2021  \n0  11.19  11.14  11.13  11.16  11.18  11.15  11.22  11.71  13.28  \n1   7.37   7.37   7.39   7.41   7.41   7.42   7.42   8.33   8.53  \n2  15.87  18.05  17.19  15.42  13.62  12.30  11.47  13.33  11.82  \n3   2.04   1.91   1.77   1.64   2.46   2.35   2.23   3.19   3.36  \n4   7.10   7.27   7.52   8.11   8.35   9.22   9.84  11.46  10.90  \n\n\n\nInstrucciones\n\nImprime la media y las desviaci칩n est치ndar d elas tasas de paro de cada a침o (en ese orden).\n\n\n# Print the mean and standard deviation of rates by year\nprint(unemployment[\n    ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].agg(['mean', 'std']))\n\n          2010      2011      2012      2013      2014      2015      2016  \\\nmean  8.409286  8.315440  8.317967  8.344780  8.179670  8.058901  7.925879   \nstd   6.248887  6.266795  6.367270  6.416041  6.284241  6.161170  6.045439   \n\n          2017      2018      2019      2020  \nmean  7.668626  7.426429  7.243736  8.420934  \nstd   5.902152  5.818915  5.696573  6.040915  \n\n\n\nImprime la media y la desviaci칩n est치ndar (en ese orden) de las tasas de paro de cada a침o agrupadas por continente.\n\n\n# Print yearly mean and standard deviation grouped by continent\nprint(unemployment[\n    ['continent', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020']\n].groupby(\"continent\").agg(['mean', 'std']))\n\n                    2010                 2011                 2012            \\\n                    mean       std       mean       std       mean       std   \ncontinent                                                                      \nAfrica          9.343585  7.411259   9.369245  7.401556   9.240755  7.264542   \nAsia            6.240638  5.146175   5.942128  4.779575   5.835319  4.756904   \nEurope         11.008205  6.392063  10.947949  6.539538  11.325641  7.003527   \nNorth America   8.663333  5.115805   8.563333  5.377041   8.448889  5.495819   \nOceania         3.622500  2.054721   3.647500  2.008466   4.103750  2.723118   \nSouth America   6.870833  2.807058   6.518333  2.801577   6.410833  2.936508   \n\n                    2013                 2014            ...      2016  \\\n                    mean       std       mean       std  ...      mean   \ncontinent                                                ...             \nAfrica          9.132453  7.309285   9.121321  7.291359  ...  9.277547   \nAsia            5.852128  4.668405   5.853191  4.681301  ...  6.094894   \nEurope         11.466667  6.969209  10.971282  6.759765  ...  9.394615   \nNorth America   8.840556  6.081829   8.512222  5.801927  ...  7.941111   \nOceania         3.980000  2.640119   3.976250  2.659205  ...  3.877500   \nSouth America   6.335000  2.808780   6.347500  2.834332  ...  7.230833   \n\n                             2017                2018                2019  \\\n                    std      mean       std      mean       std      mean   \ncontinent                                                                   \nAfrica         7.459439  9.284528  7.407620  9.237925  7.358425  9.264340   \nAsia           5.051796  6.171277  5.277201  6.090213  5.409128  5.949149   \nEurope         5.822793  8.359744  5.177845  7.427436  4.738206  6.764359   \nNorth America  5.503090  7.391111  5.326446  7.281111  5.253180  7.095000   \nOceania        2.477866  3.872500  2.492834  3.851250  2.455893  3.773750   \nSouth America  3.052309  7.281667  3.398994  7.496667  3.408856  7.719167   \n\n                              2020            \n                    std       mean       std  \ncontinent                                     \nAfrica         7.455293  10.307736  7.928166  \nAsia           5.254008   7.012340  5.699609  \nEurope         4.124734   7.470513  4.071218  \nNorth America  4.770490   9.297778  4.963045  \nOceania        2.369068   4.273750  2.617490  \nSouth America  3.379845  10.275000  3.411263  \n\n[6 rows x 22 columns]\n\n\nEstos datos est치n bien resumidos, pero es un poco largo. 쯈u칠 pasar칤asi quisieras enfocarte en un resumen de solo un a침o y hacerlo m치s legible? 춰Int칠ntalo en el siguiente ejercicio!\n\n\n\nAgregaciones con nombre\nYa has visto c칩mo .groupby() y .agg() pueden combinarse para mostrar res칰menes para categor칤as. A veces, es 칰til nombrar nuevas columnas al agregar, para que se quede claro en la salida del c칩digo qu칠 agregaciones se est치n aplicando y d칩nde.\nTu tarea consiste en crear un DataFrame llamado continent_summary que muestre una fila por cada continente. Las columnas del DataFram,e contendr치n la tasa de paro media de cada continente en 2021, as칤 como la desviaci칩n est치ndar de la tasa de empleo del 2021. Y por supuesto, 춰renombrar치s las columnas para que su contenido quede claro!\n\nInstrucciones\n\nCrea una columna llamada mean_rate_2021 que muestre la tasa de paro media de 2021 para cada continente.\nCrea una columna llamada std_rate_2021 que muestre la desviaci칩n est치ndar de la tasa de paro de 2021 para cada continente.\n\n\ncontinent_sumary = unemployment[\n    ['continent', '2021']\n].groupby('continent').agg(\n    # Create the mean_rate_2021 column\n    mean_rate_2021 = ('2021', 'mean'),\n    # Create the std_rate_2021 column\n    std_rate_2021 = ('2021', 'std'),\n)\nprint(continent_sumary)\n\n               mean_rate_2021  std_rate_2021\ncontinent                                   \nAfrica              10.473585       8.131636\nAsia                 6.906170       5.414745\nEurope               7.414872       3.947825\nNorth America        9.155000       5.076482\nOceania              4.280000       2.671522\nSouth America        9.924167       3.611624\n\n\nEl desempleo promedio de 2021 vari칩 ampliamente por continente, y tambi칠n lo hizo el desempleo dentro de esos continentes.\n\n\n\nVisualizar res칰menes categ칩ricos\nComo has aprendido en este cap칤tulo, Seaborn tiene muchas visualizaciones estupendas para la exploraci칩n, incluido un gr치fico de barras para mostrar un valor medio agregado por categor칤a de datos.\nEn Seaborn, los gr치ficos de barras incluyen una barra vertical que indica el intervalo de confianza del 95 % para la media categ칩rica. Como los intervalos de confianza se calculan utilizando tanto el n칰mero de valores como la variabilidad de esos valores, dan una indicaci칩n 칰til de hasta qu칠 punto se puede confiar en los datos.\nTu tarea consiste en crear un diagrama de barras para visualizar las medias y los intervalos de confianza de las tasas de desempleo en los distintos continentes.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nCrea un diagrama de barras que muestre los continentes en el eje x y lsus respectivas tasas medias de desempleo en 2021 en el eje y.\n\n\n# Create a bar plot of continents and their average unemployment\nsns.barplot(data=unemployment, x='continent', y='2021',\n            hue='continent', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\nAunque Europa tiene un mayor desempleo promedio que Asia, tambi칠n tiene un intervalo de confianza m치s peque침o para ese promedio, por lo que el valor promedio es m치s confiable.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>1</span> <span class='chapter-title'>Conocer un conjunto de datos</span>"
    ]
  },
  {
    "objectID": "index.html#descripci칩n",
    "href": "index.html#descripci칩n",
    "title": "An치lisis Exploratorio de Datos en Python",
    "section": "Descripci칩n",
    "text": "Descripci칩n\nAs칤 que tienes algunos datos interesantes, 쯣or d칩nde empiezas tu an치lisis? Este curso cubrir치 el proceso de exploraci칩n y an치lisis de datos, desde la comprensi칩n de lo que se incluye en un conjunto de datos hasta la incorporaci칩n de los resultados de la exploraci칩n a un flujo de trabajo de ciencia de datos.\nUtilizando datos sobre cifras de desempleo y precios de billetes de avi칩n, aprovechar치s Python para resumir y validar datos, calcular, identificar y reemplazar valores perdidos, y limpiar valores num칠ricos y categ칩ricos. A lo largo del curso, crear치s hermosas visualizaciones Seaborn para comprender las variables y sus relaciones.\nPor ejemplo, examinar치s c칩mo se relacionan el consumo de alcohol y el rendimiento de los alumnos. Por 칰ltimo, el curso mostrar치 c칩mo los hallazgos exploratorios alimentan los flujos de trabajo de la ciencia de datos creando nuevas caracter칤sticas, equilibrando caracter칤sticas categ칩ricas y generando hip칩tesis a partir de los hallazgos.\nAl final de este curso, tendr치s la confianza necesaria para realizar tu propio an치lisis exploratorio de datos (EDA) en Python. 춰Ser치s capaz de explicar tus conclusiones visualmente a los dem치s y sugerir los siguientes pasos para recopilar informaci칩n a partir de tus datos!",
    "crumbs": [
      "Bienvenida"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html",
    "href": "02_Limpieza_e_imputacion_de_datos.html",
    "title": "Limpieza e imputaci칩n de datos",
    "section": "",
    "text": "Tratar los datos que faltan\nExplorar y analizar datos a menudo significa tratar con valores perdidos, tipos de datos incorrectos y valores at칤picos. En este cap칤tulo, aprender치s t칠cnicas para gestionar estos problemas y agilizar tus procesos en EDA.\nprint(salaries.isna().sum())\nthreshold = len(salaries) * 0.05\nprint(threhold)\ncols_to_drop = salaries.columns[salaries.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\nsalaries.dropna(subset=cols_to_drop, inplace=True) # Para actualizar el DataFrame\ncols_with_missing_values = salaries.columns[salaries.isna().sum() &gt; 0]\nprint(cols_with_missing_values)\nfor col in cols_with_missing_values[:-1]:\n        salaries[col].fillna(salaries[col].mode()[0])\nprint(salaries.isna().sum())\nsalaries_dict = salaries.groupby('Experience')['Salary_USD'].median().to_dict()\nprint(salaries_dict)\nsalaries['Salary_USD'] = salaries['Salary_USD'].fillna(salaries['Experience'].map(salaries_dict))",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>2</span> <span class='chapter-title'>Limpieza e imputaci칩n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categ칩ricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#convertir-y-analizar-datos-categ칩ricos",
    "title": "Limpieza e imputaci칩n de datos",
    "section": "Convertir y analizar datos categ칩ricos",
    "text": "Convertir y analizar datos categ칩ricos\n\nPrevisualizar los datos\n\n\nprint(salaries.select_dtypes('object').head())\n\n\n\nT칤tulos de los trabajos\n\n\nprint(salaries['Designation'].value_counts())\n\n\n\nprint(salaries['Designation'].nunique())\n\n\n\n\nExtrayendo valores desde las categor칤as\n\nEl formato actual de los datos limita la capacidad de generar informaci칩n.\npandas.Series.str.contains()\n\nBusca en una columna una cadena especifica o m칰ltiples cadenas.\n\n\n\n\nsalaries['Designation'].str.contains('Scientist')\n\n\n\nFiltrar filas que contienen una o m치s frases\n\nPalabras de interes: Machine Learning o AI\n\n\n\nsalaries['Designation'].str.contains('Machine Learning|AI')\n\n\n\nBuscar m칰ltiples frases en una cadena de caracteres\n\nPalabras de interes: Cualquiera que inicie con Data\n\n\n\nsalaries['Designation'].str.contains('틙Data')\n\n\nAhora que se tiene una idea de c칩mo funciona este m칠todo, definamos una lista de t칤tulos de trabajo que queremos encontrar:\n\njob_categories = ['Data Science', 'Data Analytics',\n                   'Data Engineering', 'Machine Learning',\n                   'Managerial', 'Consultant']\n\nLuego necesitamos crear variables que contengan nuestros filtros\n\ndata_science = 'Data Scientist|NLP'\ndata_analyst = 'Analyst|Analytics'\ndata_engineer = 'Data Engineer|ETL|Architect|Infrastructure'\nml_engineer = 'Machine Learning|ML|Bid Data|AI'\nmanager = 'Manager|Head|Director|Lead|Principal|Staff'\nconsultant = 'Consultant|Freelance'\n\nEl siguiente paso es crear una lista con nuestro rango de condiciones para el m칠todo str.contains\n\nconditions = [\n    (salaries['Designation'].str.contains(data_science)),\n    (salaries['Designation'].str.contains(data_analyst)),\n    (salaries['Designation'].str.contains(data_engineer)),\n    (salaries['Designation'].str.contains(ml_engineer)),\n    (salaries['Designation'].str.contains(manager)),\n    (salaries['Designation'].str.contains(consultant))\n]\n\nFinalmente, podemos crear nuestra nueva columna Job_Category usando la funci칩n de selecci칩n de Numpy\n\nsalaries['Job_Category'] = np.select(conditions,\n                                     job_categories,\n                                     default='Other')\n\nAl obtener una vista previa de la Designaci칩n y nuestra nueva columna Job_Category, podemos verificar los primeros cinco valores.\n\nprint(salaries[['Designation', 'Job_Category']].head())\n\n\n\nVisualizaci칩n de la frecuencia de la categor칤a job\n\n\nsns.countplot(data=salaries, x='Job_Category')\nplt.show()\n\n\n\nEncontrar el n칰mero de valores 칰nicos\nTe gustar칤a practicar algunas de las habilidades de manipulaci칩n y an치lisis de datos categ칩ricos que acabas de ver. Para ayudarte a identificar qu칠 datos podr칤an reformatearse para extraer valor, vas a averiguar qu칠 columnas no num칠ricas del conjunto de datos planes tienen un gran n칰mero de valores 칰nicos.\n\nInstrucciones\n\nFiltra planes para las columnas que sean del tipo datos \"object\".\nRecorre las columnas del conjunto de datos.\nA침ade el iterador de columna a la sentencia print y, a continucaci칩n, llama a la funci칩n para que devuelva el n칰mero de valores 칰nicos de la columna.\n\n\n# Filter the DataFrame for objects columns\nnon_numeric = planes.select_dtypes('object')\n\n# Loop through columns\nfor column in non_numeric.columns:\n    # Print the number of unique values\n    print(f\"Number of unique values in {column} column: {non_numeric[column].nunique()}\")\n\nNumber of unique values in Airline column: 8\nNumber of unique values in Date_of_Journey column: 44\nNumber of unique values in Source column: 5\nNumber of unique values in Destination column: 6\nNumber of unique values in Route column: 122\nNumber of unique values in Dep_Time column: 218\nNumber of unique values in Arrival_Time column: 1220\nNumber of unique values in Duration column: 362\nNumber of unique values in Total_Stops column: 5\n\n\nCuriosamente, \"Duration\" es actualmente una columna de tipo objeto cuando deber칤a ser una columna num칠rica, 춰y tiene 362 valores 칰nicos! Vamos a averiguar m치s sobre esta columna.\n\n\n\nCategor칤a de duraci칩n de vuelos\nComo has visto, hay 362 valores 칰nicos en la columna \"Duration\" de planes. Llamando a planes['Duration'].head(), vemos los siguientes valores.\n\n\n0        19h\n1     5h 25m\n2     4h 45m\n3     2h 25m\n4    15h 30m\nName: Duration, dtype: object\n\n\nParece que no ser치 sencillo convertirlo a n칰meros. Sin embargo, 춰podr칤as clasificar los vuelos por duraci칩n y examinar la frecuencia de las distintas longitudes de vuelo!\nCrear치s una columna \"Duration_Category\" en el DataFrame planes. Antes tendr치s que crear una lista de valores que deseas insertar en el DataFrame, seguida de los valores existentes a partir de los cuales deben crearse.\n\nInstrucciones\n\nCrea una lista de categor칤as que contengan \"Short-haul\", \"Medium\" y \"Long-haul\".\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n\n\n\n\nCrea short_flights, una cadena para capturar valores de \"0h\", \"1h\", \"2h\", \"3h\", \"4h\" teniendo cuidado de evitar valores como \"10h\".\nCrea medium_flights para capturar cualquier valor entre cinco y nueve horas. ~\nCrea long_flights para capturar cualquier valor comprendido entre 10 y 16 horas, ambos inclusive.\n\n\n# Create a list of categories\nflight_categories = ['Short-haul', 'Medium', 'Long-haul']\n\n# Create short-haul values\nshort_flights = '^0h|^1h|^2h|^3h|^4h'\n\n# Create medium-haul values\nmedium_flights = '^5h|^6h|^7h|^8h|^9h'\n\n# Create long-haul values\nlong_flights = '^10h|^11h|^12h|^13h|^14h|^15h|^16h'\n\nAhora has creado tus categor칤as y valores, es hora de agregar condicionalmente las categor칤as en el DataFrame\n\n\n\nA침adir categor칤as de duraci칩n\nAhora que has configurado las categor칤as y los valores que quieres capturar, 춰es hora de construir una nueva columna para analizar la frecuencia de los vuelos seg칰n su duraci칩n!\nLas variablesflight_categories, short_flights, medium_flights y long_flights que creaste anteriormente est치n a tu disposici칩n.\n\nimport numpy as np\n\n\nInstrucciones\n\nCrea conditions, una lista que contenga subconjuntos de planes['Duration'] basados en short_flights, medium_flights y long_flights.\nCrea la columna \"Duration_Category\" llamando a una funci칩n que acepte tu lista conditions y flight_categories, estableciendo los valores no encontrados en \"Extreme duration\".\nCrea un gr치fico fque muestre el recuento de cada categor칤a.\n\n\n# Create conditions for values in flight_categories to be created\nconditions = [\n    (planes['Duration'].str.contains(short_flights)),\n    (planes['Duration'].str.contains(medium_flights)),\n    (planes['Duration'].str.contains(long_flights))\n]\n\n# Apply the conditions list to the flight_categories\nplanes['Duration_Category'] = np.select(conditions, flight_categories,\n                                        default='Extreme duration')\n\n# Plot the counts of each category륲nsns.countplot(data=planes, x='Duration_Category',\n              hue='Duration_Category', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\n춰Est치 claro que la mayor칤a de los vuelos son de corta distancia.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>2</span> <span class='chapter-title'>Limpieza e imputaci칩n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-num칠ricos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#trabajar-con-datos-num칠ricos",
    "title": "Limpieza e imputaci칩n de datos",
    "section": "Trabajar con datos num칠ricos",
    "text": "Trabajar con datos num칠ricos\n\nEl dataset origina de los salarios\n\n\nprint(salaries.info())\n\n\n\nSalario en Rupias\n\n\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nConvirtiendo cadena de caracteres en n칰meros\n\nRemover las comas de los valores Salary_In_Rupees\nConvertir la columna a tipo de dato float\nCrear una nueva columna convirtiendo la moneda a d칩lares\n\n\n\npd.Series.str.replace('Caracter a remover', 'Caracter a reemplazar')\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].str.replace(',', '')\nprint(salaries['Salary_In_Rupees'].head())\n\n\n\nsalaries['Salary_In_Rupees'] = salaries['Salary_In_Rupees'].astype(float) \n\n1 Indian Rupee = 0.012 US Dollars\n\nsalaries['Salary_USD'] = salaries['Salary_In_Rupees'] * 0.012\n\n\nPrevisualizando la nueva columna\n\n\nprint(salaries[['Salary_In_Rupees', 'Salary_USD']].head())\n\n\n\nA침adiendo un resumen esrtad칤stico al DataFrame\n\n\nsalaries.groupby('Company_Size')['Salary_USD'].mean()\n\n\nCalculo de la desviaci칩n est치ndar de los salarios por experiencia:\n\n\nsalaries['std_dev'] = salaries.groupby('Experience') \\ \n                      ['Salary_USD'].transform(lambda x: x.std())\n\n\nprint(salaries[['Experience', 'std_dev']].value_counts())\n\n\nRepitamos el proceso para otros datos estad칤sticos:\n\nsalaries['median_by_comp_size'] = salaries.groupby('Company_Size') \\\n                                  ['Salary_USD'].transform(lambda x: x.median())\n\n\nprint(salaries[['Company_Size', 'median_by_comp_size']].head())\n\n\n\nDuraci칩n del vuelo\nTe gustar칤a analizar la duraci칩n de los vuelos, pero por desgracia, la columna \"Duration\" de DataFrame planes contiene actualmente valores de cadena.\nTendr치s que limpiar la columna y convertirla al tipo de datos correcto para el an치lisis.\n\nimport re\n\ndef duration_to_decimal_str(duration_str: str) -&gt; str:\n    '''\n    Convierte una duraci칩n de vuelo de formato '2h 30m' a una cadena en formato decimal en horas, como '2.5h'.\n    \n    Par치metros:\n    -----------\n    duration_str : str\n        Cadena de texto que representa la duraci칩n de un vuelo, como '2h 30m', '45m', '19h', etc.\n\n    Retorna:\n    --------\n    str\n        Cadena con duraci칩n expresada en horas decimales, con un solo decimal y el sufijo 'h'. Ej: '2.5h'\n    '''\n    horas = re.search(r'(\\d+)\\s*h', duration_str)\n    minutos = re.search(r'(\\d+)\\s*m', duration_str)\n\n    h = int(horas.group(1)) if horas else 0\n    m = int(minutos.group(1)) if minutos else 0\n\n    decimal_hours = round(h + m / 60, 1)\n    return f'{decimal_hours}h'\n\n\n\nplanes['Duration'] = planes['Duration'].apply(duration_to_decimal_str)\n\n\nInstrucciones\n\nImprime los cinco primeros valores de la columna \"Duration\".\n\n\n# Preview the column\nprint(planes['Duration'].head())\n\n0    19.0h\n1     5.4h\n2     4.8h\n3     2.4h\n4    15.5h\nName: Duration, dtype: object\n\n\n\nRetira \"h\" de la columna\n\n\n# Remove the string character\nplanes['Duration'] = planes['Duration'].str.replace('h', '')\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: object\n\n\n\nConvierte la columna al tipo de datos float.\n\n\n# Convert to float data type\nplanes['Duration'] = planes['Duration'].astype(float)\nprint(planes['Duration'].head())\n\n0    19.0\n1     5.4\n2     4.8\n3     2.4\n4    15.5\nName: Duration, dtype: float64\n\n\n\nTraza un histograma de los valores de \"Duration\"\n\n\n# Plot a histogram\nsns.histplot(data=planes, x='Duration')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nA침adir estad칤sticas descriptivas\nAhora \"Duration\" y \"Price\"contienen valores num칠ricos en el DataFrame planes, y te gustar칤a calcular para ellos estad칤sticas de resumen condicionadas a los valores de otras columnas.\n\nInstrucciones\n\nA침ade una columna a planes que contenga la desviaci칩n est치ndar de \"Price\" basada en \"Airline\".\n\n\n# Price standard deviation by Airline\nplanes['airline_price_st_dev'] = planes.groupby('Airline')['Price'].transform(lambda x: x.std())\nprint(planes[['Airline', 'airline_price_st_dev']].value_counts())\n\nAirline            airline_price_st_dev\nJet Airways        4159.846432             3082\nIndiGo             2245.529140             1632\nAir India          3692.609285             1399\nMultiple carriers  3558.323763              959\nSpiceJet           1798.900648              653\nVistara            2888.915498              376\nAir Asia           1979.826234              260\nGoAir              2764.926625              147\nName: count, dtype: int64\n\n\n\nCalcula la mediana de \"Duration\" en \"Airline\", almacen치ndola como una columna llamada \"airline_median_duration\".\n\n\n# Median Duration by Airline\nplanes['airline_median_duration'] = planes.groupby('Airline')['Duration'].transform(lambda x: x.median())\nprint(planes[['Airline', 'airline_median_duration']].value_counts())\n\nAirline            airline_median_duration\nJet Airways        13.3                       3082\nIndiGo             2.9                        1632\nAir India          15.5                       1399\nMultiple carriers  10.2                        959\nSpiceJet           2.5                         653\nVistara            3.2                         376\nAir Asia           2.8                         260\nGoAir              2.9                         147\nName: count, dtype: int64\n\n\n\nEncuenta la media \"Price\" por \"Destination\", guard치ndola como una columna llamada \"price_destination_mean\".\n\n\n# Mean Price by Destination\nplanes['price_destination_mean'] = planes.groupby('Destination')['Price'].transform(lambda x: x.mean())\nprint(planes[['Destination', 'price_destination_mean']].value_counts())\n\nDestination  price_destination_mean\nCochin       10473.585927              3631\nBanglore     9093.622872               2291\nDelhi        5248.541082                998\nNew Delhi    11579.306944               720\nHyderabad    5190.274021                562\nKolkata      4907.156863                306\nName: count, dtype: int64\n\n\nParece que Jet Airways tiene la mayor desviaci칩n est치ndar en precio, Air India tiene la mayor duraci칩n median, y Nueva Delhi, en promedio, es el destiono m치s caro. Ahora veamos c칩mo manejar los datos at칤picos.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>2</span> <span class='chapter-title'>Limpieza e imputaci칩n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#gesti칩n-de-valores-at칤picos",
    "href": "02_Limpieza_e_imputacion_de_datos.html#gesti칩n-de-valores-at칤picos",
    "title": "Limpieza e imputaci칩n de datos",
    "section": "Gesti칩n de valores at칤picos",
    "text": "Gesti칩n de valores at칤picos\n\nQu칠 es un outlier?\n\nEs una observaci칩n que est치 muy alejada de otros puntos de datos.\n\nUsando estad칤stica descriptiva\n\n\nprint(salaries['Salary_USD'].describe())\n\n\n\nUsando el rango intercuartil\n\nRango intercuartil (IQR)\n\nIQR = 75th - 25th percentil\nUpper outliers &gt; 75th percentile + (1.5 * IQR)\nLower Outliers &lt; 25th percentile - (1.5 * IQR)\n\n\n\n\nsns.boxplot(data=salaries, y='Salaary_USD')\nplt.show()\n\n\n\nIdentificando Umbrales\n\n\n# 75th percentil\nseventy_fifth = salaries['Salary_USD'].quantile(0.75)\n\n# 25th percentil\ntwenty_fifth = salaries['Salary_USD'].quantile(0.25)\n\n# Interquartil range\nsalaries_iqr = seventy_fifth - twenty_fifth\n\nprint(salaries_iqr)\n\n\n\n# Upper threshold\nupper = seventy_fifth + (1.5 * salaries_iqr)\n\n# Lower threshold\nlower = twenty_fifth - (1.5 * salaries_iqr)\n\nprint(upper, lower)\n\n\n\nSubdividiendo nuestros datos\n\n\nsalaries[(salaries['Salary_USD'] &lt; lower) | (salaries['Salary_USD'] &gt; upper)] \\\n        [['Experience', 'Employee_Location', 'Salary_USD']]\n\n\n\n Por qu칠 buscar los Outliers?\n\nLos Outliers son valores extremos\n\nPueden no representar con precisi칩n los datos\n\nPueden sesgar la media y la desviaci칩n est치ndar\nPruebas de estad칤stica y modelos de machine learning requieren datos que tengan una distribuci칩n normal y no esten sesgados.\n\nQu칠 hacer con los Outliers?\n\nPreguntas que nos debemos hacer:\n\nPor qu칠 existen los outliers?\nLos valores son precisos?\n\n\nEliminaci칩n de Outliers\n\n\nno_outliers = salaries[(salaries['Salar_USD'] &gt; lower) & (salaries['Salary_USD'] &lt; upper)]\n\n\nprint(no_outliers['Salary_USD'].describe())\n\n\n\nDistribuci칩n de Salarios",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>2</span> <span class='chapter-title'>Limpieza e imputaci칩n de datos</span>"
    ]
  },
  {
    "objectID": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "href": "02_Limpieza_e_imputacion_de_datos.html#tratar-los-datos-que-faltan",
    "title": "Limpieza e imputaci칩n de datos",
    "section": "",
    "text": "Por qu칠 un dato faltante es un problema?\n\nAfectan las distribuciones\nLos datos de la poblaci칩n son menos repreesntativos\nPuede resultar en conclusiones incorrectas\n\nEjemplo datos de profesionales de datos\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nData type\n\n\n\n\nWorking_Year\nYear the data was obtained\nFloat\n\n\nDesignation\nJob title\nString\n\n\nExperience\nExperience level e.g., \"Mid\", \"Senior\"\nString\n\n\nEmployment_Satus\nType of employment contract e.g., \"FT\", \"PT\"\nString\n\n\nEmployee_Location\nCountry of employment\nString\n\n\nCompany_Size\nLabels for company size e.g., \"S\", \"M\", \"L\"\nString\n\n\nRemote_Working_Ratio\nPorcentage of time working remotely\nInteger\n\n\nSalary_USD\nSalary in US dollars\nFloat\n\n\n\n\nRevisando los datos faltantes\n\n\n\n\nEstrategias para el manejo de datos faltantes\n\nEliminar los datos faltantes\n\n5 % o menos del total de valores\n\nImputar la media, mediana o la moda\n\nDepende de la distribuci칩n y contexto\n\nImputar por sub-grupos\n\nDiferentes niveles de experiencia tienen diferente mediana en el salario\n\n\nEliminando valores faltantes\n\n\n\n\n\n\n\nImputando una estad칤stica de resumen\n\n\n\n\n\nRevisando los valores faltantes que faltan\n\n\n\n\nImputando por subgrupo\n\n\n\n\n\n\nTratar los datos que faltan\nEs importante tratar los datos que faltan antes de empezar el an치lisis.\nUn enfoque consiste en descartar los valores que faltan si representan una peque침a proporci칩n, normalmente el 5 %, de los datos.\nTrabajando con un conjunto de datos sobre precios de tiquetes de avi칩n, almacenado como un DataFrame de pandas llamado planes, tendr치s que contar el n칰mero de valores perdidos en todas las columnas, calcular el cinco porciento de todos los valores, utilizar este umbral para eliminar observaciones y comprobar cu치ntos valores perdidos quedan en el conjunto de datos.\n\nimport pandas as pd\n\nruta = './data/planes.csv'\nplanes = pd.read_csv(ruta)\nprint(planes.head)\n\n&lt;bound method NDFrame.head of            Airline Date_of_Journey    Source Destination  \\\n0      Jet Airways       9/06/2019     Delhi      Cochin   \n1           IndiGo      12/05/2019   Kolkata    Banglore   \n2           IndiGo      01/03/2019  Banglore   New Delhi   \n3         SpiceJet      24/06/2019   Kolkata    Banglore   \n4      Jet Airways      12/03/2019  Banglore   New Delhi   \n...            ...             ...       ...         ...   \n10655     Air Asia       9/04/2019   Kolkata    Banglore   \n10656    Air India      27/04/2019   Kolkata    Banglore   \n10657  Jet Airways      27/04/2019  Banglore       Delhi   \n10658      Vistara      01/03/2019  Banglore   New Delhi   \n10659    Air India       9/05/2019     Delhi      Cochin   \n\n                       Route Dep_Time  Arrival_Time Duration Total_Stops  \\\n0      DEL  LKO  BOM  COK    09:25  04:25 10 Jun      19h     2 stops   \n1            CCU  NAG  BLR    18:05         23:30   5h 25m      1 stop   \n2            BLR  NAG  DEL    16:50         21:35   4h 45m      1 stop   \n3                  CCU  BLR    09:00         11:25   2h 25m    non-stop   \n4            BLR  BOM  DEL    18:55  10:25 13 Mar  15h 30m      1 stop   \n...                      ...      ...           ...      ...         ...   \n10655              CCU  BLR    19:55         22:25   2h 30m    non-stop   \n10656              CCU  BLR    20:45         23:20   2h 35m    non-stop   \n10657              BLR  DEL      NaN         11:20       3h    non-stop   \n10658              BLR  DEL    11:30         14:10   2h 40m    non-stop   \n10659  DEL  GOI  BOM  COK    10:55         19:15   8h 20m     2 stops   \n\n                   Additional_Info    Price  \n0                          No info  13882.0  \n1                          No info   6218.0  \n2                          No info  13302.0  \n3                          No info   3873.0  \n4      In-flight meal not included  11087.0  \n...                            ...      ...  \n10655                      No info   4107.0  \n10656                      No info   4145.0  \n10657                          NaN   7229.0  \n10658                      No info  12648.0  \n10659                      No info  11753.0  \n\n[10660 rows x 11 columns]&gt;\n\n\n\nInstrucciones\n\nImprime el n칰mero de valores perdidos en cada columna del DataFrame\n\n\n# Count the number of missing values in each column\nprint(planes.isna().sum())\n\nAirline            427\nDate_of_Journey    322\nSource             187\nDestination        347\nRoute              256\nDep_Time           260\nArrival_Time       194\nDuration           214\nTotal_Stops        212\nAdditional_Info    589\nPrice              616\ndtype: int64\n\n\n\nCalcula a cu치ntas observaciones equivale el cinco porciento del DataFrame planes\n\n\n# Find the five percent threshold\nthreshold = len(planes) * 0.05\nprint(threshold)\n\n533.0\n\n\n\n\n\n\nCrea cols_to_drop aplicando una indexaci칩n booleana a las columnas del DataFrame con valores perdidos menores o iguales que el umbral.\nUtiliza este filtro para eliminar los valores que faltan y guardar el DataFrame actualizado.\n\n\n# Create a filter\ncols_to_drop = planes.columns[planes.isna().sum() &lt;= threshold]\nprint(cols_to_drop)\n\n# Drop missing values for columns below the threshold\nplanes.dropna(subset=cols_to_drop, inplace=True)\n\nprint(planes.isna().sum())\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops'],\n      dtype='object')\nAirline              0\nDate_of_Journey      0\nSource               0\nDestination          0\nRoute                0\nDep_Time             0\nArrival_Time         0\nDuration             0\nTotal_Stops          0\nAdditional_Info    300\nPrice              368\ndtype: int64\n\n\nAl crear un umbral de valores faltantes y usarlo para filtrar columnas, haz logrado eliminar los valores faltantes de todas las columnas excepto \"Additinal_Info\" y \"Price\".\n\n\n\nEstrategias para datos que faltan\nLa regla del cinco porciento ha funcionado muy bien en tu conjunto de dato planes, 춰eliminando los valores perdidos de nueve de las 11 columnas!\nAhora tienes que decidir qu칠 hacer con las columnas \"Additional_Info\" y \"Price\", a las que les faltan los valores 300 y 368 respectivamente.\nPrimero echar치s un vistazo a lo que contiene \"Additional_Info\", y despu칠s visualizar치s el precio de los billetes de avi칩n de distintas compa침칤as a칠reas.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nInstrucciones\n\nImprime los valores y frecuencias de \"Additional_Info\".\n\n\n# Check the values of the Additional_Info column\nprint(planes['Additional_Info'].value_counts())\n\nAdditional_Info\nNo info                         6399\nIn-flight meal not included     1525\nNo check-in baggage included     258\n1 Long layover                    14\nChange airports                    7\nNo Info                            2\nBusiness class                     1\nRed-eye flight                     1\n2 Long layover                     1\nName: count, dtype: int64\n\n\n\nCrea un boxplot de \"Price\" frente a \"Airline\"\n\n\n# Create a box plot of Price by Airline\nsns.boxplot(data=planes, x='Airline', y='Price',\n            hue='Airline', legend=False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nPregunta\n\n\n쮺칩mo debes tratar los valores que faltan en \"Additional_Info\" y \"Price\".?\n\nRespuestas Posibles\n\nElimina la columna \"Additional_Info\" e imputa la media para los valores que faltan de \"Price\".\nElimina los valores de \"No info\" de \"Additiona_Info\" e imputa la mediana de los valores que faltan de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la media por \"Airline\" para los valores que falten de \"Price\".\nElimina la columna \"Additional_Info\" e imputa la mediana por \"Airline\" para los valores que falten de \"Price\".\n\nNo necesitamos la columna \"Additional_Info\", y deber칤as imputar la mediana de \"Price\" por \"Airline\" para representar los datos con precisi칩n.\n\n\n\nImputar los precios de los aviones que faltan\n!Ahora solo queda una columna con valores perdidos!\nHas eliminado la columna \"Additional_Info\" de planes, el 칰ltimo paso es imputar los datos que faltan en la columna \"Price\" del conjunto de datos.\nComo recordatorio, t칰 generaste este diagrama de caja, que suger칤a que imputar el precio medio bas치ndose en el \"Airline\" 춰es un enfoque s칩lido!\n\n# Eliminamos la columna Additional_Info\nplanes = planes.drop('Additional_Info', axis=1)\nplanes.columns\n\nIndex(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops', 'Price'],\n      dtype='object')\n\n\n\nInstrucciones\n\nAgrupa planes por aerol칤nea y calcula el precio medio.\n\n\n# Calculate median plane ticket prices by Airplane\nairline_prices = planes.groupby('Airline')['Price'].median()\n\nprint(airline_prices)\n\nAirline\nAir Asia              5192.0\nAir India             9443.0\nGoAir                 5003.5\nIndiGo                5054.0\nJet Airways          11507.0\nMultiple carriers    10197.0\nSpiceJet              3873.0\nVistara               8028.0\nName: Price, dtype: float64\n\n\n\nConvierte los precios medios agrupados en un diccionario.\n\n\n# Convert to a dictionary\nprices_dict = airline_prices.to_dict()\nprint(prices_dict)\n\n{'Air Asia': 5192.0, 'Air India': 9443.0, 'GoAir': 5003.5, 'IndiGo': 5054.0, 'Jet Airways': 11507.0, 'Multiple carriers': 10197.0, 'SpiceJet': 3873.0, 'Vistara': 8028.0}\n\n\n\n\n\n\nImputa condicionalmente los valores perdidos de \"Price\" asignando los valores de la columna \"Airline\" en funci칩n de prices_dict\nComprueba si faltan valores\n\n\n# Map the dictionary to missing values of Price by Airline\nplanes['Price'] = planes['Price'].fillna(planes['Airline'].map(prices_dict))\n\n# Check for missing values\nprint(planes.isna().sum())\n\nAirline            0\nDate_of_Journey    0\nSource             0\nDestination        0\nRoute              0\nDep_Time           0\nArrival_Time       0\nDuration           0\nTotal_Stops        0\nPrice              0\ndtype: int64\n\n\nConvertiste un DataFrame agrupado a un diccionario y luego lo usaste para llenar condicionalmente los valores faltantes de \"Price\" bas치ndote en \"Airline\". Ahora vamos a explorar c칩mo realizar an치lisis exploratorio en datos categ칩ricos.",
    "crumbs": [
      "Cap칤tulos",
      "<span class='chapter-number'>2</span> <span class='chapter-title'>Limpieza e imputaci칩n de datos</span>"
    ]
  }
]